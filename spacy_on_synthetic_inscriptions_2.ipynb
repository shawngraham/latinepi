{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ok, used my latinepi (which needs cleaning up) to download *a lot* of inscriptions from edh. Then used a combination of scripts & llm to annotate them. In this notebook, we clean out the unsuccessfully annotated inscriptions, leaving us with a core of real data, and then we generate a whole bunch of synthetic inscriptions (thank god for formulaic epigraphy, eh?) that are correctly annotated, and mix them both together. The goal is to train the latinCy spaCy model to recognize the elements of funerary inscriptions, for data extraction from transcriptions. Why not?"
      ],
      "metadata": {
        "id": "rEmjAaQYVHdH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ufyr6JarIeFz"
      },
      "outputs": [],
      "source": [
        "!mkdir assets         # To store your raw data files (jsonl, csv)\n",
        "!mkdir configs        # To store configuration files\n",
        "!mkdir scripts        # To store helper scripts (like data conversion)\n",
        "!mkdir training       # To store the output of the training process\n",
        "!mkdir corpus         # To store the processed .spacy files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U spacy #already in colab\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#!pip install \"la-core-web-sm @ https://huggingface.co/latincy/la_core_web_sm/resolve/main/la_core_web_sm-any-py3-none-any.whl\"\n",
        "!pip install \"la-core-web-lg @ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl\"\n",
        "#\n",
        "# this is what we're going to retrain.\n",
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7rqCltXoIgt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then you have to run this. It will say things have crashed. Ignore and continue.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "1vtGdT_tIlE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# don't use any of this, just generate some fresh ones below\n",
        "# start with some synthethic training annotations\n",
        "\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/2121b3ee828c4547fc2fd9470158e6d8/raw/ea0e2c578097ea67ab6c54e170d4559ea7c29790/simple-training.jsonl -O assets/synthetic-training.jsonl ## simple ones\n",
        "\n",
        "\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/f44663efc80916a75c736a38f024b371/raw/6585104793170cb5ef7c57dde16adf2d591dff04/synthetic-training.jsonl -O assets/synthetic-training.jsonl\n",
        "\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/f44663efc80916a75c736a38f024b371/raw/a924e157c8d87d13377dcc93e890879251d0c674/synthetic-training.jsonl # this is a mixture of 400+ lines of completely synthetic data and 1000+ lines of real data with llm generated annotations\n",
        "\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/f44663efc80916a75c736a38f024b371/raw/9b1d724d19b30ded168268af0fd959dccaae521e/synthetic-training.jsonl -O assets/synthethic-training.jsonl\n",
        "\n",
        "## and some synthetic testing data\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/3633224a209ab01f650f9dee9183888d/raw/9cc9dfeb566dc8465d744e6745af98af363a227c/testing-epigraphs-synthetic.csv -O assets/test-fake-epigraphs.csv\n",
        "\n",
        "## Real inscriptions from EDH\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/8229265886e776624476331194c79934/raw/dac32c34cfb8528edaf1c2d961ed8b8f77e24c86/inscriptions.csv -O assets/inscriptions.csv"
      ],
      "metadata": {
        "id": "T3nrliM9Im0c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean up some real inscriptions to add with the synthetic ones"
      ],
      "metadata": {
        "id": "reytWfT4SkkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### ok, try to clean up some real ones and then mix them in with the synthetic\n",
        "\n",
        "## this is a couple hundred rows of real inscriptions that were annotated\n",
        "## through combination of scripts & llm, but the results had issues with\n",
        "## annotation offsets\n",
        "!wget https://gist.githubusercontent.com/shawngraham/d949119d45f5cc661205a3bfbb266d86/raw/c93dfe9324d01478fe88f59f9cf9ee9d560dfc1e/annotations-to-clean-up-for-dataset.jsonl -O assets/to-clean.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-9PwnTCNCRf",
        "outputId": "04a4eaad-ab4f-44eb-9bf0-4823d3442142"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-21 14:11:51--  https://gist.githubusercontent.com/shawngraham/d949119d45f5cc661205a3bfbb266d86/raw/c93dfe9324d01478fe88f59f9cf9ee9d560dfc1e/annotations-to-clean-up-for-dataset.jsonl\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 459000 (448K) [text/plain]\n",
            "Saving to: ‘assets/to-clean.jsonl’\n",
            "\n",
            "assets/to-clean.jso 100%[===================>] 448.24K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-11-21 14:11:51 (18.5 MB/s) - ‘assets/to-clean.jsonl’ saved [459000/459000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## so we clean out the ones that are no good, just keeps the cleanest/easiest to fix\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "def clean_real_inscriptions(input_path, output_path, model='la_core_web_lg'):\n",
        "    \"\"\"\n",
        "    Cleans real inscription data:\n",
        "    1. Copies transcription → text\n",
        "    2. Re-validates all annotation spans\n",
        "    3. Drops records with errors or unalignable annotations\n",
        "    4. Outputs clean subset ready for training\n",
        "    \"\"\"\n",
        "    nlp = spacy.load(model)\n",
        "\n",
        "    stats = {\n",
        "        \"total\": 0,\n",
        "        \"has_error_flag\": 0,\n",
        "        \"no_transcription\": 0,\n",
        "        \"no_annotations\": 0,\n",
        "        \"perfect_match\": 0,\n",
        "        \"fixed_spans\": 0,\n",
        "        \"dropped_records\": 0,\n",
        "        \"saved\": 0\n",
        "    }\n",
        "\n",
        "    salvaged = []\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "                stats[\"total\"] += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Skip flagged errors\n",
        "            if record.get(\"_error\"):\n",
        "                stats[\"has_error_flag\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Use transcription as text\n",
        "            text = record.get('transcription', '').strip()\n",
        "            if not text:\n",
        "                stats[\"no_transcription\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Update record to use text field\n",
        "            record['text'] = text\n",
        "            record.pop('transcription', None)  # Remove redundant field\n",
        "\n",
        "            # Skip if no annotations\n",
        "            annotations = record.get('annotations', [])\n",
        "            if not isinstance(annotations, list) or not annotations:\n",
        "                stats[\"no_annotations\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Validate/fix annotations\n",
        "            doc = nlp.make_doc(text)\n",
        "            validated_ents = []\n",
        "            record_is_salvageable = True\n",
        "\n",
        "            for entity in annotations:\n",
        "                if not isinstance(entity, list) or len(entity) != 3:\n",
        "                    continue\n",
        "\n",
        "                start, end, label = entity\n",
        "\n",
        "                # Validate span is within text bounds\n",
        "                if start < 0 or end > len(text) or start >= end:\n",
        "                    record_is_salvageable = False\n",
        "                    break\n",
        "\n",
        "                # Check if span matches actual text\n",
        "                span_text = text[start:end]\n",
        "\n",
        "                # Try alignment\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "\n",
        "                if span is not None:\n",
        "                    # Span aligned successfully\n",
        "                    if span.start_char == start and span.end_char == end:\n",
        "                        # Perfect match\n",
        "                        validated_ents.append([start, end, label])\n",
        "                        stats[\"perfect_match\"] += 1\n",
        "                    else:\n",
        "                        # Adjusted but acceptable\n",
        "                        validated_ents.append([span.start_char, span.end_char, label])\n",
        "                        stats[\"fixed_spans\"] += 1\n",
        "                else:\n",
        "                    # Could not align - record is bad\n",
        "                    record_is_salvageable = False\n",
        "                    break\n",
        "\n",
        "            if record_is_salvageable and validated_ents:\n",
        "                record['annotations'] = validated_ents\n",
        "                salvaged.append(record)\n",
        "                stats[\"saved\"] += 1\n",
        "            else:\n",
        "                stats[\"dropped_records\"] += 1\n",
        "\n",
        "    # Save cleaned data\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for record in salvaged:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"\\n✅ Real inscription cleaning complete\")\n",
        "    print(f\"   Total records: {stats['total']}\")\n",
        "    print(f\"   Flagged as error: {stats['has_error_flag']}\")\n",
        "    print(f\"   No transcription: {stats['no_transcription']}\")\n",
        "    print(f\"   No annotations: {stats['no_annotations']}\")\n",
        "    print(f\"   Perfect annotations: {stats['perfect_match']}\")\n",
        "    print(f\"   Fixed spans: {stats['fixed_spans']}\")\n",
        "    print(f\"   Dropped (unrecoverable): {stats['dropped_records']}\")\n",
        "    print(f\"   ✅ SAVED: {stats['saved']} clean records\")\n",
        "    print(f\"   Saved to: {output_path}\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "# Run it\n",
        "clean_real_inscriptions('assets/to-clean.jsonl', 'assets/real_inscriptions_clean.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6Va11flNKvj",
        "outputId": "0cc92524-0146-4141-8441-8bb634811f27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Real inscription cleaning complete\n",
            "   Total records: 1100\n",
            "   Flagged as error: 221\n",
            "   No transcription: 0\n",
            "   No annotations: 9\n",
            "   Perfect annotations: 2111\n",
            "   Fixed spans: 7932\n",
            "   Dropped (unrecoverable): 175\n",
            "   ✅ SAVED: 695 clean records\n",
            "   Saved to: assets/real_inscriptions_clean.jsonl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total': 1100,\n",
              " 'has_error_flag': 221,\n",
              " 'no_transcription': 0,\n",
              " 'no_annotations': 9,\n",
              " 'perfect_match': 2111,\n",
              " 'fixed_spans': 7932,\n",
              " 'dropped_records': 175,\n",
              " 'saved': 695}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## make some synthetic"
      ],
      "metadata": {
        "id": "bBRXbtgyTcG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Generate synthetic Latin inscriptions with:\n",
        "- Weighted name selection (based on real data frequencies)\n",
        "- Focused label set: TRIBE, PRAENOMEN, NOMEN, COGNOMEN, EPITHET, DEDICATORY_FORMULA, ORIGIN\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class FrequencyExtractor:\n",
        "    \"\"\"Extract frequency distributions from real inscription data\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_frequencies(input_path):\n",
        "        \"\"\"Extract praenomen, nomen, cognomen frequencies from real data\"\"\"\n",
        "        praenomen_freq = Counter()\n",
        "        nomen_freq = Counter()\n",
        "        cognomen_freq = Counter()\n",
        "        tribe_freq = Counter()\n",
        "        origin_freq = Counter()\n",
        "\n",
        "        with open(input_path) as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                text = record.get('text', '')\n",
        "                annotations = record.get('annotations', [])\n",
        "\n",
        "                if not text or not isinstance(annotations, list):\n",
        "                    continue\n",
        "\n",
        "                for entity in annotations:\n",
        "                    if isinstance(entity, list) and len(entity) == 3:\n",
        "                        start, end, label = entity\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    if start < 0 or end > len(text):\n",
        "                        continue\n",
        "\n",
        "                    word = text[start:end].strip()\n",
        "\n",
        "                    if label == 'PRAENOMEN':\n",
        "                        praenomen_freq[word] += 1\n",
        "                    elif label == 'NOMEN':\n",
        "                        nomen_freq[word] += 1\n",
        "                    elif label == 'COGNOMEN':\n",
        "                        cognomen_freq[word] += 1\n",
        "                    elif label == 'TRIBE':\n",
        "                        tribe_freq[word] += 1\n",
        "                    elif label == 'ORIGIN':\n",
        "                        origin_freq[word] += 1\n",
        "\n",
        "        return praenomen_freq, nomen_freq, cognomen_freq, tribe_freq, origin_freq\n",
        "\n",
        "    @staticmethod\n",
        "    def get_weighted_list(freq_counter, top_n=30):\n",
        "        \"\"\"Convert frequency counter to weighted selection list\"\"\"\n",
        "        # Get top N items, weighted by frequency\n",
        "        items = [word for word, _ in freq_counter.most_common(top_n)]\n",
        "        return items if items else []\n",
        "\n",
        "\n",
        "class FocusedInscriptionGenerator:\n",
        "    \"\"\"Generate inscriptions focused on 7 core labels\"\"\"\n",
        "\n",
        "    def __init__(self, praenomina, nomina, cognomina, tribes, origins):\n",
        "        self.PRAENOMINA = praenomina or ['M', 'L', 'C', 'T', 'Q']\n",
        "        self.NOMINA = nomina or ['IVLIVS', 'CLAVDIVS', 'FLAVIVS']\n",
        "        self.COGNOMINA = cognomina or ['VICTOR', 'FELIX', 'MAXIMVS']\n",
        "        self.TRIBES = tribes or ['GAL', 'POL', 'VEL']\n",
        "        self.ORIGINS = origins or ['Roma', 'Italia', 'Hispania', 'Gallia', 'Britannia']\n",
        "\n",
        "        self.RELATIONSHIPS = ['CONIVGI', 'FILIO', 'FILIAE', 'PATRI', 'FRATRI', 'SORORI']\n",
        "        self.EPITHETS = ['BENE MERENTI', 'PIISSIMO', 'DVLCISSIMO', 'CARISSIMO', 'OPTIMO']\n",
        "\n",
        "        self.DATIVE_MAP = {\n",
        "            'IVLIVS': 'IVLIO', 'CLAVDIVS': 'CLAVDIO', 'FLAVIVS': 'FLAVIO',\n",
        "            'VICTOR': 'VICTORI', 'FELIX': 'FELICI', 'MAXIMVS': 'MAXIMO',\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_roman(num):\n",
        "        \"\"\"Convert integer to Roman numeral.\"\"\"\n",
        "        roman_map = [(100, 'C'), (90, 'XC'), (50, 'L'), (40, 'XL'), (10, 'X'),\n",
        "                     (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')]\n",
        "        result = ''\n",
        "        for val, sym in roman_map:\n",
        "            count = num // val\n",
        "            result += sym * count\n",
        "            num -= val * count\n",
        "        return result\n",
        "\n",
        "    def _to_dative(self, latin_word):\n",
        "        \"\"\"Convert nominative Latin name to dative case.\"\"\"\n",
        "        return self.DATIVE_MAP.get(latin_word, latin_word)\n",
        "\n",
        "    def generate_inscriptions(self, count=1000):\n",
        "        \"\"\"Generate focused inscriptions\"\"\"\n",
        "        data = []\n",
        "\n",
        "        for i in range(count):\n",
        "            spans = []\n",
        "            text_parts = []\n",
        "\n",
        "            # Optional: Dedicatory formula header (D M = Dis Manibus) - 70% chance\n",
        "            if random.random() < 0.7:\n",
        "                start = 0\n",
        "                text_parts.append(\"D M\")\n",
        "                spans.append([start, 3, \"DEDICATORY_FORMULA\"])\n",
        "\n",
        "            # Deceased person - Praenomen + Nomen + Cognomen (all in dative)\n",
        "            prae_dec = random.choice(self.PRAENOMINA)\n",
        "            nomen_dec = random.choice(self.NOMINA)\n",
        "            cog_dec = random.choice(self.COGNOMINA)\n",
        "\n",
        "            # Mark praenomen (dative)\n",
        "            start = len(\" \".join(text_parts)) + 1 if text_parts else 0\n",
        "            text_parts.append(prae_dec)\n",
        "            spans.append([start, start + len(prae_dec), \"PRAENOMEN\"])\n",
        "\n",
        "            # Mark nomen (dative)\n",
        "            nomen_dat = self._to_dative(nomen_dec)\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(nomen_dat)\n",
        "            spans.append([start, start + len(nomen_dat), \"NOMEN\"])\n",
        "\n",
        "            # Optional tribe (30% chance)\n",
        "            if random.random() < 0.3:\n",
        "                tribe = random.choice(self.TRIBES)\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(tribe)\n",
        "                spans.append([start, start + len(tribe), \"TRIBE\"])\n",
        "\n",
        "            # Mark cognomen (dative) - 90% include it\n",
        "            if random.random() < 0.9:\n",
        "                cog_dat = self._to_dative(cog_dec)\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(cog_dat)\n",
        "                spans.append([start, start + len(cog_dat), \"COGNOMEN\"])\n",
        "\n",
        "            # Optional age at death (60% chance)\n",
        "            if random.random() < 0.6:\n",
        "                years = random.randint(15, 90)\n",
        "                roman_num = self._to_roman(years)\n",
        "                age_string = f\"vixit ann(is) {roman_num}\"\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(age_string)\n",
        "                # Age is not labeled - just part of text for context\n",
        "\n",
        "            # Optional origin (40% chance) - 70% with deceased, 30% independent\n",
        "            if random.random() < 0.4:\n",
        "                origin = random.choice(self.ORIGINS)\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(f\"ex {origin}\")\n",
        "                spans.append([start + 3, start + 3 + len(origin), \"ORIGIN\"])\n",
        "\n",
        "            # Optional dedicator (50% chance)\n",
        "            if random.random() < 0.5:\n",
        "                ded_prae = random.choice(self.PRAENOMINA)\n",
        "                ded_nomen = random.choice(self.NOMINA)\n",
        "                ded_cog = random.choice(self.COGNOMINA)\n",
        "\n",
        "                # Add each component separately\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(ded_prae)\n",
        "                spans.append([start, start + len(ded_prae), \"PRAENOMEN\"])\n",
        "\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(ded_nomen)\n",
        "                spans.append([start, start + len(ded_nomen), \"NOMEN\"])\n",
        "\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(ded_cog)\n",
        "                spans.append([start, start + len(ded_cog), \"COGNOMEN\"])\n",
        "\n",
        "                # Optional epithet with dedicator (50% chance)\n",
        "                if random.random() < 0.5:\n",
        "                    epithet = random.choice(self.EPITHETS)\n",
        "                    start = len(\" \".join(text_parts)) + 1\n",
        "                    text_parts.append(epithet)\n",
        "                    spans.append([start, start + len(epithet), \"EPITHET\"])\n",
        "\n",
        "                # Optional action verb (60% chance)\n",
        "                if random.random() < 0.6:\n",
        "                    actions = ['posuit', 'fecit', 'dedicavit']\n",
        "                    action = random.choice(actions)\n",
        "                    start = len(\" \".join(text_parts)) + 1\n",
        "                    text_parts.append(action)\n",
        "                    # Action not labeled\n",
        "\n",
        "            # Construct final text\n",
        "            full_text = \" \".join(text_parts)\n",
        "\n",
        "            # Create entry\n",
        "            entry = {\n",
        "                \"id\": f\"gen_{i+1:04d}\",\n",
        "                \"text\": full_text,\n",
        "                \"annotations\": spans\n",
        "            }\n",
        "            data.append(json.dumps(entry))\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Extract frequencies from real data\n",
        "    print(\"Extracting frequencies from real inscriptions...\")\n",
        "    praenomina_freq, nomina_freq, cognomina_freq, tribe_freq, origin_freq = \\\n",
        "        FrequencyExtractor.extract_frequencies('assets/real_inscriptions_clean.jsonl')\n",
        "\n",
        "    # Get weighted lists\n",
        "    praenomina = FrequencyExtractor.get_weighted_list(praenomina_freq, top_n=20)\n",
        "    nomina = FrequencyExtractor.get_weighted_list(nomina_freq, top_n=30)\n",
        "    cognomina = FrequencyExtractor.get_weighted_list(cognomina_freq, top_n=40)\n",
        "    tribes = FrequencyExtractor.get_weighted_list(tribe_freq, top_n=15)\n",
        "    origins = FrequencyExtractor.get_weighted_list(origin_freq, top_n=20)\n",
        "\n",
        "    print(f\"✅ Extracted frequencies:\")\n",
        "    print(f\"   Praenomina: {len(praenomina)} unique\")\n",
        "    print(f\"   Nomina: {len(nomina)} unique\")\n",
        "    print(f\"   Cognomina: {len(cognomina)} unique\")\n",
        "    print(f\"   Tribes: {len(tribes)} unique\")\n",
        "    print(f\"   Origins: {len(origins)} unique\")\n",
        "\n",
        "    # Generate synthetic data\n",
        "    print(\"\\nGenerating 3000 synthetic inscriptions with weighted selection...\")\n",
        "    gen = FocusedInscriptionGenerator(praenomina, nomina, cognomina, tribes, origins)\n",
        "    inscriptions = gen.generate_inscriptions(3000)\n",
        "\n",
        "    # Write to file\n",
        "    with open('assets/synthetic_focused.jsonl', 'w') as f:\n",
        "        for inscription in inscriptions:\n",
        "            f.write(inscription + '\\n')\n",
        "\n",
        "    print(f\"✅ Saved to assets/synthetic_focused.jsonl\")\n",
        "\n",
        "    # Show sample\n",
        "    print(\"\\nSample inscriptions:\")\n",
        "    with open('assets/synthetic_focused.jsonl') as f:\n",
        "        for _ in range(3):\n",
        "            sample = json.loads(f.readline())\n",
        "            print(f\"\\n  Text: {sample['text']}\")\n",
        "            print(f\"  Labels: {[ann[2] for ann in sample['annotations']]}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjPUNOzHLpWf",
        "outputId": "222704dc-c75c-4283-dad1-f0b81e3d3dcc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting frequencies from real inscriptions...\n",
            "✅ Extracted frequencies:\n",
            "   Praenomina: 20 unique\n",
            "   Nomina: 30 unique\n",
            "   Cognomina: 40 unique\n",
            "   Tribes: 15 unique\n",
            "   Origins: 0 unique\n",
            "\n",
            "Generating 3000 synthetic inscriptions with weighted selection...\n",
            "✅ Saved to assets/synthetic_focused.jsonl\n",
            "\n",
            "Sample inscriptions:\n",
            "\n",
            "  Text: Tito Cornelio Galeria Fortunati vixit ann(is) XXX Publi Claudiae Rufus DVLCISSIMO fecit\n",
            "  Labels: ['PRAENOMEN', 'NOMEN', 'TRIBE', 'COGNOMEN', 'PRAENOMEN', 'NOMEN', 'COGNOMEN', 'EPITHET']\n",
            "\n",
            "  Text: Tiberius Marcus Fortunati Aulus Caecilius filius\n",
            "  Labels: ['PRAENOMEN', 'NOMEN', 'COGNOMEN', 'PRAENOMEN', 'NOMEN', 'COGNOMEN']\n",
            "\n",
            "  Text: D M Luci Lucio Arnensi Secundo ex Italia Tiberius Attius nini Pii\n",
            "  Labels: ['DEDICATORY_FORMULA', 'PRAENOMEN', 'NOMEN', 'TRIBE', 'COGNOMEN', 'ORIGIN', 'PRAENOMEN', 'NOMEN', 'COGNOMEN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## combine real & synthethic"
      ],
      "metadata": {
        "id": "QrrC2vGtS63j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2 harmonize real & synthetic\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Harmonize all data to focused 7-label schema:\n",
        "- PRAENOMEN\n",
        "- NOMEN\n",
        "- COGNOMEN\n",
        "- TRIBE\n",
        "- EPITHET\n",
        "- DEDICATORY_FORMULA\n",
        "- ORIGIN\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "REAL_LABEL_MAPPING = {\n",
        "    # Direct mappings\n",
        "    \"PRAENOMEN\": \"PRAENOMEN\",\n",
        "    \"NOMEN\": \"NOMEN\",\n",
        "    \"COGNOMEN\": \"COGNOMEN\",\n",
        "    \"TRIBE\": \"TRIBE\",\n",
        "    \"EPITHET\": \"EPITHET\",\n",
        "    \"ORIGIN\": \"ORIGIN\",\n",
        "\n",
        "    # Consolidate formulas\n",
        "    \"FUNERARY_FORMULA\": \"DEDICATORY_FORMULA\",\n",
        "    \"DEDICATION_TO_THE_GODS\": \"DEDICATORY_FORMULA\",\n",
        "    \"BENE_MERENTI\": \"EPITHET\",\n",
        "\n",
        "    # Drop these labels (too noisy/rare)\n",
        "    \"FILIATION\": None,\n",
        "    \"RELATIONSHIP\": None,\n",
        "    \"AGE_YEARS\": None,\n",
        "    \"AGE_DAYS\": None,\n",
        "    \"AGE_MONTHS\": None,\n",
        "    \"AGE_PREFIX\": None,\n",
        "    \"OCCUPATION\": None,\n",
        "    \"MILITARY_UNIT\": None,\n",
        "    \"AGE\": None,\n",
        "    \"ACTION\": None,\n",
        "    \"VERB\": None,\n",
        "    \"DECEASED_NAME\": None,\n",
        "}\n",
        "\n",
        "\n",
        "def harmonize_real_data(input_path, output_path):\n",
        "    \"\"\"Harmonize real inscriptions to 7-label schema\"\"\"\n",
        "    harmonized = []\n",
        "    dropped = 0\n",
        "\n",
        "    with open(input_path) as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            text = record.get('text', '')\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            annotations = record.get('annotations', [])\n",
        "            if not isinstance(annotations, list):\n",
        "                continue\n",
        "\n",
        "            # Filter and remap annotations\n",
        "            new_annotations = []\n",
        "            for entity in annotations:\n",
        "                if not isinstance(entity, list) or len(entity) != 3:\n",
        "                    continue\n",
        "\n",
        "                start, end, label = entity\n",
        "\n",
        "                # Remap label\n",
        "                new_label = REAL_LABEL_MAPPING.get(label)\n",
        "\n",
        "                if new_label is None:\n",
        "                    # Drop this label\n",
        "                    dropped += 1\n",
        "                    continue\n",
        "\n",
        "                # Validate span\n",
        "                if start < 0 or end > len(text) or start >= end:\n",
        "                    continue\n",
        "\n",
        "                new_annotations.append([start, end, new_label])\n",
        "\n",
        "            # Only keep records with at least one annotation\n",
        "            if new_annotations:\n",
        "                record['annotations'] = new_annotations\n",
        "                harmonized.append(record)\n",
        "\n",
        "    # Write output\n",
        "    with open(output_path, 'w') as f:\n",
        "        for record in harmonized:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"✅ Harmonized real inscriptions\")\n",
        "    print(f\"   Records saved: {len(harmonized)}\")\n",
        "    print(f\"   Annotations dropped: {dropped}\")\n",
        "\n",
        "\n",
        "def combine_and_split(real_path, synthetic_path, train_out, dev_out, real_weight=0.2):\n",
        "    \"\"\"Combine real + synthetic and split into train/dev\"\"\"\n",
        "\n",
        "    # Load data\n",
        "    real_records = []\n",
        "    with open(real_path) as f:\n",
        "        real_records = [json.loads(line) for line in f]\n",
        "\n",
        "    synthetic_records = []\n",
        "    with open(synthetic_path) as f:\n",
        "        synthetic_records = [json.loads(line) for line in f]\n",
        "\n",
        "    # Combine\n",
        "    combined = real_records + synthetic_records\n",
        "\n",
        "    print(f\"\\n✅ Combined data:\")\n",
        "    print(f\"   Real: {len(real_records)} ({len(real_records)/len(combined)*100:.1f}%)\")\n",
        "    print(f\"   Synthetic: {len(synthetic_records)} ({len(synthetic_records)/len(combined)*100:.1f}%)\")\n",
        "    print(f\"   Total: {len(combined)}\")\n",
        "\n",
        "    # Split\n",
        "    train, dev = train_test_split(combined, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Write\n",
        "    with open(train_out, 'w') as f:\n",
        "        for record in train:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    with open(dev_out, 'w') as f:\n",
        "        for record in dev:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"\\n✅ Split into train/dev:\")\n",
        "    print(f\"   Train: {len(train)}\")\n",
        "    print(f\"   Dev: {len(dev)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Harmonize real data\n",
        "    print(\"=== HARMONIZING REAL DATA ===\")\n",
        "    harmonize_real_data('assets/real_inscriptions_clean.jsonl',\n",
        "                       'assets/real_focused.jsonl')\n",
        "\n",
        "    # Combine and split\n",
        "    print(\"\\n=== COMBINING AND SPLITTING ===\")\n",
        "    combine_and_split('assets/real_focused.jsonl',\n",
        "                     'assets/synthetic_focused.jsonl',\n",
        "                     'assets/train_focused.jsonl',\n",
        "                     'assets/dev_focused.jsonl',\n",
        "                     real_weight=0.2)\n",
        "\n",
        "    print(\"\\n✅ Ready for alignment and training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiWNnvEYSHwr",
        "outputId": "471d4241-2181-4717-b8b5-dd1b1901bb92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== HARMONIZING REAL DATA ===\n",
            "✅ Harmonized real inscriptions\n",
            "   Records saved: 627\n",
            "   Annotations dropped: 4142\n",
            "\n",
            "=== COMBINING AND SPLITTING ===\n",
            "\n",
            "✅ Combined data:\n",
            "   Real: 627 (17.3%)\n",
            "   Synthetic: 3000 (82.7%)\n",
            "   Total: 3627\n",
            "\n",
            "✅ Split into train/dev:\n",
            "   Train: 2901\n",
            "   Dev: 726\n",
            "\n",
            "✅ Ready for alignment and training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Align and convert to .spacy\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "def align_annotations(input_path, output_path):\n",
        "    nlp = spacy.load('la_core_web_lg')\n",
        "    corrected_records = []\n",
        "\n",
        "    stats = {\"total\": 0, \"perfect\": 0, \"fixed\": 0, \"malformed\": 0, \"dropped\": 0}\n",
        "\n",
        "    with open(input_path, 'r') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "                stats[\"total\"] += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            text = record.get('text', '')\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            annotations = record.get('annotations', [])\n",
        "            doc = nlp.make_doc(text)\n",
        "            corrected_ents = []\n",
        "\n",
        "            for entity in annotations:\n",
        "                if isinstance(entity, list) and len(entity) == 3:\n",
        "                    start, end, label = entity\n",
        "                else:\n",
        "                    stats[\"malformed\"] += 1\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    start, end = int(start), int(end)\n",
        "                except (ValueError, TypeError):\n",
        "                    stats[\"malformed\"] += 1\n",
        "                    continue\n",
        "\n",
        "                if start < 0 or end > len(text) or start >= end:\n",
        "                    stats[\"malformed\"] += 1\n",
        "                    continue\n",
        "\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "\n",
        "                if span is not None:\n",
        "                    if span.start_char == start and span.end_char == end:\n",
        "                        stats[\"perfect\"] += 1\n",
        "                    else:\n",
        "                        stats[\"fixed\"] += 1\n",
        "                    corrected_ents.append([span.start_char, span.end_char, label])\n",
        "                else:\n",
        "                    stats[\"dropped\"] += 1\n",
        "\n",
        "            record['annotations'] = corrected_ents\n",
        "            corrected_records.append(record)\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        for record in corrected_records:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"\\n✅ Aligned {input_path}\")\n",
        "    print(f\"   Perfect: {stats['perfect']}, Fixed: {stats['fixed']}, Dropped: {stats['dropped']}\")\n",
        "\n",
        "# Align both\n",
        "align_annotations('assets/train_focused.jsonl', 'assets/train_focused_aligned.jsonl')\n",
        "align_annotations('assets/dev_focused.jsonl', 'assets/dev_focused_aligned.jsonl')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enXTp35tNSr5",
        "outputId": "83b7f12c-ec3f-4d34-ead8-95a6e72b3aa1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Aligned assets/train_focused.jsonl\n",
            "   Perfect: 17280, Fixed: 120, Dropped: 21\n",
            "\n",
            "✅ Aligned assets/dev_focused.jsonl\n",
            "   Perfect: 4447, Fixed: 32, Dropped: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Convert to .spacy format\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "def create_spacy_file(input_path, output_path, model='la_core_web_lg'):\n",
        "    nlp = spacy.load(model)\n",
        "    db = DocBin()\n",
        "\n",
        "    with open(input_path) as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line)\n",
        "            text = record.get('text', '')\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            doc = nlp.make_doc(text)\n",
        "            ents = []\n",
        "\n",
        "            for start, end, label in record.get('annotations', []):\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "                if span:\n",
        "                    ents.append(span)\n",
        "\n",
        "            doc.ents = filter_spans(ents)\n",
        "            db.add(doc)\n",
        "\n",
        "    db.to_disk(output_path)\n",
        "    print(f\"✅ Created {output_path}\")\n",
        "\n",
        "create_spacy_file('assets/train_focused_aligned.jsonl', 'corpus/train.spacy')\n",
        "create_spacy_file('assets/dev_focused_aligned.jsonl', 'corpus/dev.spacy')\n",
        "\n",
        "print(\"\\n✅ Ready for training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEuAI4amusm5",
        "outputId": "fddb7e96-5cdb-427a-dc30-25a7d208c2c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created corpus/train.spacy\n",
            "✅ Created corpus/dev.spacy\n",
            "\n",
            "✅ Ready for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Check sample output\n",
        "with open('assets/synthetic_focused.jsonl') as f:\n",
        "    samples = [json.loads(f.readline()) for _ in range(3)]\n",
        "    print(\"\\nSample synthetic inscriptions:\")\n",
        "    for sample in samples:\n",
        "        print(f\"\\n  {sample['text']}\")\n",
        "        labels = [ann[2] for ann in sample['annotations']]\n",
        "        print(f\"  Labels: {set(labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEK1LZWuuu6e",
        "outputId": "bb595e5d-a746-4c7c-9c61-d2337774697b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample synthetic inscriptions:\n",
            "\n",
            "  Tito Cornelio Galeria Fortunati vixit ann(is) XXX Publi Claudiae Rufus DVLCISSIMO fecit\n",
            "  Labels: {'COGNOMEN', 'TRIBE', 'EPITHET', 'PRAENOMEN', 'NOMEN'}\n",
            "\n",
            "  Tiberius Marcus Fortunati Aulus Caecilius filius\n",
            "  Labels: {'COGNOMEN', 'NOMEN', 'PRAENOMEN'}\n",
            "\n",
            "  D M Luci Lucio Arnensi Secundo ex Italia Tiberius Attius nini Pii\n",
            "  Labels: {'TRIBE', 'COGNOMEN', 'PRAENOMEN', 'ORIGIN', 'DEDICATORY_FORMULA', 'NOMEN'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Generate the base config ---\n",
        "!python -m spacy init config configs/config.cfg --lang la --pipeline tok2vec,ner --optimize accuracy --force\n",
        "\n",
        "print(\"✅ Base 'config.cfg' generated.\")\n",
        "\n",
        "# --- 2. Load and Modify ---\n",
        "config_path = Path(\"configs/config.cfg\")\n",
        "config = spacy.util.load_config(config_path)\n",
        "\n",
        "# Define the model we are using\n",
        "LATIN_MODEL = \"la_core_web_lg\"\n",
        "\n",
        "# --- Part A: Initialize Vectors (CRITICAL FOR LG MODELS) ---\n",
        "# This loads the 300-dim vectors into the vocab so the tok2vec layer can find them.\n",
        "config[\"initialize\"][\"vectors\"] = LATIN_MODEL\n",
        "\n",
        "# --- Part B: Source the tok2vec component ---\n",
        "config[\"components\"][\"tok2vec\"] = {\n",
        "    \"source\": LATIN_MODEL,\n",
        "    \"component\": \"tok2vec\"\n",
        "}\n",
        "\n",
        "# --- Part C: Connect NER to the vectors ---\n",
        "# ERROR CORRECTION: The tok2vec OUTPUT width is 96, even if the input vectors are 300.\n",
        "config[\"components\"][\"ner\"][\"model\"][\"tok2vec\"] = {\n",
        "    \"@architectures\": \"spacy.Tok2VecListener.v1\",\n",
        "    \"width\": 96,  # <--- Reverted to 96. This matches the output of la_core_web_lg.\n",
        "    \"upstream\": \"tok2vec\"\n",
        "}\n",
        "\n",
        "config[\"nlp\"][\"batch_size\"] = 200\n",
        "\n",
        "# --- Part D: Paths and Freezing ---\n",
        "config[\"paths\"][\"train\"] = \"./corpus/train.spacy\"\n",
        "config[\"paths\"][\"dev\"] = \"./corpus/dev.spacy\"\n",
        "\n",
        "# Freeze tok2vec so we don't ruin the pretrained Latin intelligence\n",
        "#config[\"training\"][\"frozen_components\"] = [\"tok2vec\"]\n",
        "# or unfreeze it, see what happens\n",
        "config[\"training\"][\"frozen_components\"] = []\n",
        "#config[\"training\"][\"max_epochs\"]= 100\n",
        "config[\"training\"][\"max_epochs\"] = 5\n",
        "# Mark it as annotating so it actually runs\n",
        "config[\"training\"][\"annotating_components\"] = [\"tok2vec\"]\n",
        "\n",
        "# --- 3. Save ---\n",
        "config.to_disk(config_path)\n",
        "\n",
        "print(f\"✅ Config updated for {LATIN_MODEL}. Listener width set to 96 (correct output dim).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mKD51UtI_uY",
        "outputId": "ce347f9e-74a3-4257-de59-53fd5d0c0e32"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: la\n",
            "- Pipeline: ner\n",
            "- Optimize for: accuracy\n",
            "- Hardware: CPU\n",
            "- Transformer: None\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "configs/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "✅ Base 'config.cfg' generated.\n",
            "✅ Config updated for la_core_web_lg. Listener width set to 96 (correct output dim).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training process!\n",
        "!python -m spacy train configs/config.cfg --output ./training/ #--gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXBY8quJEL5",
        "outputId": "857466c5-8536-46bf-f7c4-012334a4cb96"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: training\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Set annotations on update for: ['tok2vec']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     69.33   12.19   15.28   10.14    0.12\n",
            "  0     200        205.46   4614.68   88.87   93.87   84.37    0.89\n",
            "  1     400        216.32   2682.71   88.79   92.78   85.14    0.89\n",
            "  2     600        297.30   2872.65   88.36   91.27   85.63    0.88\n",
            "  3     800        384.43   2750.82   88.53   90.74   86.42    0.89\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.scorer import Scorer\n",
        "from spacy.training import Example\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "def evaluate_final(model_path, dev_data_path):\n",
        "    print(f\"--- Evaluating {model_path} ---\")\n",
        "    nlp = spacy.load(model_path)\n",
        "    db = DocBin().from_disk(dev_data_path)\n",
        "    docs = list(db.get_docs(nlp.vocab))\n",
        "\n",
        "    examples = []\n",
        "    for doc in docs:\n",
        "        examples.append(Example(nlp(doc.text), doc))\n",
        "\n",
        "    scores = Scorer().score(examples)\n",
        "\n",
        "    print(f\"{'LABEL':<30} {'PREC':<8} {'REC':<8} {'F1':<8}\")\n",
        "    print(\"-\" * 60)\n",
        "    for label, metrics in scores['ents_per_type'].items():\n",
        "        print(f\"{label:<30} {metrics['p']:.2f}     {metrics['r']:.2f}     {metrics['f']:.2f}\")\n",
        "\n",
        "evaluate_final(\"training/model-best\", \"corpus/dev.spacy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11NHuAmJJKl4",
        "outputId": "eece507c-699f-46ef-ef85-0cc60fa1ef33"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating training/model-best ---\n",
            "LABEL                          PREC     REC      F1      \n",
            "------------------------------------------------------------\n",
            "PRAENOMEN                      0.95     0.97     0.96\n",
            "COGNOMEN                       0.93     0.87     0.90\n",
            "NOMEN                          0.94     0.90     0.92\n",
            "DEDICATORY_FORMULA             0.92     0.56     0.70\n",
            "ORIGIN                         1.00     1.00     1.00\n",
            "EPITHET                        0.99     0.74     0.85\n",
            "TRIBE                          0.84     0.89     0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import spacy\n",
        "\n",
        "# The best model should be saved as model-best during training\n",
        "# Load it\n",
        "nlp = spacy.load(\"training/model-best\")\n",
        "\n",
        "# Test on real inscriptions from your dataset\n",
        "test_cases = [\n",
        "    \"Dis Manibus Lucio Ocratio Corrintho vixit annos XXX dies XI Ocratia Silvana filio piissimo bene merenti fecit\",\n",
        "    \"Caius Pompeius Caius libertus librarius\",\n",
        "    \"Dis Manibus sacrum Fortunatus municipum municipii Ipolcobulensiorum servus annorum XXXXIII pius\",\n",
        "    \"Dis Manibus Spediae Luci filiae Severae coniugi Luci Valeri Montani Quinti fili primi pili legionis XIII\",\n",
        "]\n",
        "\n",
        "print(\"Testing on real inscriptions:\\n\")\n",
        "for text in test_cases:\n",
        "    doc = nlp(text)\n",
        "    print(f\"Text: {text[:70]}...\")\n",
        "    print(\"Entities predicted:\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"  {ent.text:25} → {ent.label_:15}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQiq0hK9V8ip",
        "outputId": "890c52bd-b004-4417-9e6e-656890ee76a0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on real inscriptions:\n",
            "\n",
            "Text: Dis Manibus Lucio Ocratio Corrintho vixit annos XXX dies XI Ocratia Si...\n",
            "Entities predicted:\n",
            "  Dis Manibus               → DEDICATORY_FORMULA\n",
            "  Lucio                     → NOMEN          \n",
            "  Ocratio                   → NOMEN          \n",
            "\n",
            "Text: Caius Pompeius Caius libertus librarius...\n",
            "Entities predicted:\n",
            "  Caius                     → PRAENOMEN      \n",
            "  Pompeius                  → NOMEN          \n",
            "\n",
            "Text: Dis Manibus sacrum Fortunatus municipum municipii Ipolcobulensiorum se...\n",
            "Entities predicted:\n",
            "  Dis Manibus               → DEDICATORY_FORMULA\n",
            "\n",
            "Text: Dis Manibus Spediae Luci filiae Severae coniugi Luci Valeri Montani Qu...\n",
            "Entities predicted:\n",
            "  Dis Manibus               → DEDICATORY_FORMULA\n",
            "  Spediae                   → NOMEN          \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real Data Time\n",
        "\n",
        "Loading in data from P."
      ],
      "metadata": {
        "id": "b3bp9U-xyfDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Process Leiden conventions\n",
        "import csv\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "class LeidenProcessor:\n",
        "    \"\"\"Convert Leiden conventions to clean transcription\"\"\"\n",
        "\n",
        "    # Map abbreviations to their expansions\n",
        "    # Capitalize proper nouns, keep others lowercase\n",
        "    ABBREV_PROPER = {  # Names (capitalize)\n",
        "        'Q': 'Quintus', 'C': 'Caius', 'M': 'Marcus', 'L': 'Lucius',\n",
        "        'T': 'Titus', 'P': 'Publius', 'D': 'Dis', 'A': 'Aulus',\n",
        "        'Cn': 'Gnaeus', 'TI': 'Tiberius', 'S': 'Sextus', 'N': 'Numerius',\n",
        "    }\n",
        "\n",
        "    ABBREV_COMMON = {  # Common words (lowercase)\n",
        "        'a': 'animo', 'l': 'libens', 'v': 'votum', 'p': 'posuit',\n",
        "        's': 'sacrum', 'f': 'fecit', 'm': 'mensis', 'an': 'anno',\n",
        "        'ann': 'annorum', 'h': 'hic', 'e': 'est', 'pos': 'posuit',\n",
        "        't': 'tibi', 'd': 'de', 'sit': 'sit'\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def process(leiden_text):\n",
        "        \"\"\"Full pipeline: Leiden → clean transcription\"\"\"\n",
        "\n",
        "        # Step 1: Remove damage markers [3] (n unknown letters)\n",
        "        text = re.sub(r'\\[\\d+\\]', '', leiden_text)\n",
        "\n",
        "        # Step 2: Remove question marks and uncertain markers\n",
        "        text = re.sub(r'\\?', '', text)\n",
        "        text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)  # [text] → text\n",
        "\n",
        "        # Step 3: Join words broken across lines intelligently\n",
        "        # Handle patterns like \"Gem/ellian\" or \"ann]or/um\"\n",
        "        # Remove line breaks only when joining word fragments\n",
        "        text = re.sub(r'([a-z])/([a-z])', r'\\1\\2', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'(\\])/([a-z])', r'\\1\\2', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Step 4: Expand abbreviations with proper case handling\n",
        "        def expand_abbrev(match):\n",
        "            abbrev = match.group(1)\n",
        "            expansion = match.group(2) if match.group(2) else \"\"\n",
        "\n",
        "            # If expansion provided in parentheses, use it\n",
        "            if expansion:\n",
        "                # Keep expansion as-is, preserve case\n",
        "                return abbrev + expansion\n",
        "\n",
        "            # Try proper noun abbreviations first\n",
        "            if abbrev in LeidenProcessor.ABBREV_PROPER:\n",
        "                return LeidenProcessor.ABBREV_PROPER[abbrev]\n",
        "\n",
        "            # Try common abbreviations\n",
        "            if abbrev.lower() in LeidenProcessor.ABBREV_COMMON:\n",
        "                return LeidenProcessor.ABBREV_COMMON[abbrev.lower()]\n",
        "\n",
        "            # Return original if not found\n",
        "            return abbrev\n",
        "\n",
        "        # Pattern: X(expansion) captures abbreviation and optional expansion text\n",
        "        text = re.sub(r'([A-Za-z]+)\\(([^)]*)\\)', expand_abbrev, text)\n",
        "\n",
        "        # Step 5: Clean line break markers and multiple spaces\n",
        "        text = text.replace('/', ' ')\n",
        "        text = text.replace('\\\\', ' ')\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Step 6: Remove remaining brackets and junk\n",
        "        text = re.sub(r'[\\[\\]]', '', text)\n",
        "\n",
        "        # Step 7: Capitalize only first letter and proper nouns\n",
        "        # Simple heuristic: capitalize after space, but preserve lowercase articles/prepositions\n",
        "        LOWERCASE_WORDS = {'et', 'de', 'a', 'in', 'ex', 'ab'}\n",
        "\n",
        "        words = text.split()\n",
        "        result = []\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            if i == 0:  # First word always capitalized\n",
        "                result.append(word.capitalize())\n",
        "            elif word.lower() in LOWERCASE_WORDS:\n",
        "                result.append(word.lower())\n",
        "            elif word[0].isupper():  # Already capitalized (likely a name)\n",
        "                result.append(word)\n",
        "            else:\n",
        "                result.append(word.capitalize())\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "\n",
        "# Test it\n",
        "test_cases = [\n",
        "    \"Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]\",\n",
        "    \"D(is) M(anibus) s(acrum) / Severus / Tongini / an(norum) XXI / h(ic) s(itus) e(st)\",\n",
        "    \"Fig(lina) Gem/ellian[a]\",\n",
        "    \"C[3] / N[3] / Pap(iria?) [ann]or/um L h(ic) s(itus) e(st)\",\n",
        "    \"Saturn/inus Bo/uti f(ilius) an(norum)\",\n",
        "]\n",
        "\n",
        "print(\"Updated Leiden processor:\\n\")\n",
        "for leiden in test_cases:\n",
        "    transcribed = LeidenProcessor.process(leiden)\n",
        "    print(f\"Leiden:        {leiden}\")\n",
        "    print(f\"Transcription: {transcribed}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRvdZi6Gyg2p",
        "outputId": "4a60069d-d9ee-4c01-f8f8-310bffaf0a3b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Leiden processor:\n",
            "\n",
            "Leiden:        Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]\n",
            "Transcription: Iovi Optimo Maximo Quintus Cassius Cassianus Animo Libens Votum Posuit\n",
            "\n",
            "Leiden:        D(is) M(anibus) s(acrum) / Severus / Tongini / an(norum) XXI / h(ic) s(itus) e(st)\n",
            "Transcription: Dis Manibus Sacrum Severus Tongini Annorum XXI Hic Situs Est\n",
            "\n",
            "Leiden:        Fig(lina) Gem/ellian[a]\n",
            "Transcription: Figlina Gemelliana\n",
            "\n",
            "Leiden:        C[3] / N[3] / Pap(iria?) [ann]or/um L h(ic) s(itus) e(st)\n",
            "Transcription: C N Papiria Annorum L Hic Situs Est\n",
            "\n",
            "Leiden:        Saturn/inus Bo/uti f(ilius) an(norum)\n",
            "Transcription: Saturninus Bouti Filius Annorum\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Load your CSV and process it\n",
        "import pandas as pd\n",
        "\n",
        "# Read CSV (adjust filename to match your upload)\n",
        "df = pd.read_csv('assets/actual_inscriptions.csv')  # or whatever your file is named\n",
        "\n",
        "print(f\"Loaded {len(df)} inscriptions\\n\")\n",
        "\n",
        "# Process all inscriptions\n",
        "records = []\n",
        "for idx, row in df.iterrows():\n",
        "    leiden_text = row.get('text', '') if isinstance(row, dict) else row['text']\n",
        "\n",
        "    if not leiden_text:\n",
        "        continue\n",
        "\n",
        "    transcription = LeidenProcessor.process(leiden_text)\n",
        "\n",
        "    record = {\n",
        "        \"id\": f\"actual_{idx}\",\n",
        "        \"text\": transcription,\n",
        "        \"leiden_source\": leiden_text,\n",
        "        \"annotations\": []\n",
        "    }\n",
        "    records.append(record)\n",
        "\n",
        "print(f\"Processed {len(records)} inscriptions\")\n",
        "\n",
        "# Show samples\n",
        "print(\"\\nFirst 3 samples:\")\n",
        "for rec in records[:3]:\n",
        "    print(f\"\\n  Leiden:        {rec['leiden_source']}\")\n",
        "    print(f\"  Transcription: {rec['text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK1UmicY0C6f",
        "outputId": "f3762c6e-2fb5-438e-fe91-d7f2b31ba574"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 138 inscriptions\n",
            "\n",
            "Processed 138 inscriptions\n",
            "\n",
            "First 3 samples:\n",
            "\n",
            "  Leiden:        Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]\n",
            "  Transcription: Iovi Optimo Maximo Quintus Cassius Cassianus Animo Libens Votum Posuit\n",
            "\n",
            "  Leiden:        D(is) M(anibus) s(acrum) / Sycecale / v(ixit) an(n)o m(ensibus) [V] / soror[es] / Tricism[a] / Salcea / et Veget[a]\n",
            "  Transcription: Dis Manibus Sacrum Sycecale Vixit Anno Mensibus V Sorores Tricisma Salcea et Vegeta\n",
            "\n",
            "  Leiden:        Fig(lina) Gem/ellian[a]\n",
            "  Transcription: Figlina Gemelliana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Run model inference\n",
        "import spacy\n",
        "\n",
        "# Load model\n",
        "try:\n",
        "    nlp = spacy.load(\"training/model-best\")\n",
        "    print(f\"✅ Loaded model\")\n",
        "except OSError:\n",
        "    print(\"❌ Model not found at training/model-best\")\n",
        "    print(\"   Make sure the model folder exists and is in the correct location\")\n",
        "\n",
        "# Run predictions\n",
        "predictions = []\n",
        "\n",
        "for i, record in enumerate(records, 1):\n",
        "    text = record['text']\n",
        "\n",
        "    # Run model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract entities\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append({\n",
        "            \"text\": ent.text,\n",
        "            \"label\": ent.label_,\n",
        "            \"start\": ent.start_char,\n",
        "            \"end\": ent.end_char\n",
        "        })\n",
        "\n",
        "    pred = {\n",
        "        \"id\": record['id'],\n",
        "        \"leiden_source\": record['leiden_source'],\n",
        "        \"text\": text,\n",
        "        \"entities\": entities\n",
        "    }\n",
        "    predictions.append(pred)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"  Processed {i}/{len(records)}...\")\n",
        "\n",
        "print(f\"✅ Generated predictions for {len(predictions)} inscriptions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYMhnCpM0KfG",
        "outputId": "a7c1651d-4b03-4a2d-c8d9-4f7623bd1aa7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded model\n",
            "  Processed 100/138...\n",
            "✅ Generated predictions for 138 inscriptions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Display predictions\n",
        "print(\"=\" * 100)\n",
        "print(\"PREDICTIONS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for i, pred in enumerate(predictions[:10], 1):\n",
        "    print(f\"\\n[{i}] {pred['leiden_source']}\")\n",
        "    print(f\"    Transcription: {pred['text']}\")\n",
        "    print(f\"    Entities:\")\n",
        "\n",
        "    if pred['entities']:\n",
        "        for ent in pred['entities']:\n",
        "            print(f\"      • {ent['text']:30} → {ent['label']}\")\n",
        "    else:\n",
        "        print(f\"      (none detected)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZEJcsAl0PG_",
        "outputId": "3558e6bc-3f35-48da-939b-c4b092bdbba4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "PREDICTIONS\n",
            "====================================================================================================\n",
            "\n",
            "[1] Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]\n",
            "    Transcription: Iovi Optimo Maximo Quintus Cassius Cassianus Animo Libens Votum Posuit\n",
            "    Entities:\n",
            "      • Iovi                           → DEDICATORY_FORMULA\n",
            "      • Quintus                        → PRAENOMEN\n",
            "      • Cassius                        → NOMEN\n",
            "      • Cassianus                      → COGNOMEN\n",
            "\n",
            "[2] D(is) M(anibus) s(acrum) / Sycecale / v(ixit) an(n)o m(ensibus) [V] / soror[es] / Tricism[a] / Salcea / et Veget[a]\n",
            "    Transcription: Dis Manibus Sacrum Sycecale Vixit Anno Mensibus V Sorores Tricisma Salcea et Vegeta\n",
            "    Entities:\n",
            "      • Dis Manibus                    → DEDICATORY_FORMULA\n",
            "\n",
            "[3] Fig(lina) Gem/ellian[a]\n",
            "    Transcription: Figlina Gemelliana\n",
            "    Entities:\n",
            "      (none detected)\n",
            "\n",
            "[4] D(is) M(anibus) s(acrum) / Severus / Tongini / an(norum) XXI / h(ic) s(itus) e(st) s(it) t(ibi) / t(erra) l(evis) m(ater)\n",
            "    Transcription: Dis Manibus Sacrum Severus Tongini Annorum XXI Hic Situs Est Sit Tibi Terra Levis Mater\n",
            "    Entities:\n",
            "      • Dis Manibus                    → DEDICATORY_FORMULA\n",
            "\n",
            "[5] Iovi O(ptimo) M(aximo) / C(aius) A[3]/s M[3]/s v(otum) [s(olvit) l(ibens?) a(nimo?)]\n",
            "    Transcription: Iovi Optimo Maximo Caius As Ms Votum Solvit Libens Animo\n",
            "    Entities:\n",
            "      • Iovi                           → DEDICATORY_FORMULA\n",
            "      • Caius                          → PRAENOMEN\n",
            "      • As                             → NOMEN\n",
            "\n",
            "[6] Turaesio / Turcau/di f(ilius?) et / Pans[3] / Maeilonis / f(ilii?) s(it) eis / t(erra) l(evis)\n",
            "    Transcription: Turaesio Turcaudi Filius et Pans Maeilonis Filii Sit Eis Terra Levis\n",
            "    Entities:\n",
            "      • et Pans                        → COGNOMEN\n",
            "\n",
            "[7] Caeno / Loucin[i] / f(ilius) [\n",
            "    Transcription: Caeno Loucini Filius\n",
            "    Entities:\n",
            "      • Filius                         → COGNOMEN\n",
            "\n",
            "[8] Saturn/inus Bo/uti f(ilius) an(norum) [\n",
            "    Transcription: Saturninus Bouti Filius Annorum\n",
            "    Entities:\n",
            "      • Annorum                        → COGNOMEN\n",
            "\n",
            "[9] an(norum)] / XVI h(ic) s(itus?) e(st) s(it) t(ibi) t(erra) / l(evis) Afelia avia / d(e) s(uo) f(ecit)\n",
            "    Transcription: Annorum XVI Hic Situs Est Sit Tibi Terra Levis Afelia Avia de Suo Fecit\n",
            "    Entities:\n",
            "      (none detected)\n",
            "\n",
            "[10] C[3] / N[3] / Pap(iria?) [ann]or/um L h(ic) s(itus) e(st) / t(ibi) t(erra) l(evis) [3] / [3]V[\n",
            "    Transcription: C N Papiria Annorum L Hic Situs Est Tibi Terra Levis V\n",
            "    Entities:\n",
            "      • Annorum                        → COGNOMEN\n",
            "      • Situs                          → COGNOMEN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Analyze statistics\n",
        "from collections import Counter\n",
        "\n",
        "label_counts = Counter()\n",
        "entity_counts = Counter()\n",
        "\n",
        "for pred in predictions:\n",
        "    for ent in pred['entities']:\n",
        "        label_counts[ent['label']] += 1\n",
        "        entity_counts[ent['text']] += 1\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"STATISTICS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nTotal inscriptions: {len(predictions)}\")\n",
        "print(f\"Total entities detected: {sum(len(p['entities']) for p in predictions)}\")\n",
        "\n",
        "print(f\"\\nEntities by label:\")\n",
        "for label, count in sorted(label_counts.items(), key=lambda x: -x[1]):\n",
        "    print(f\"  {label:25} {count:5}\")\n",
        "\n",
        "print(f\"\\nMost common entities:\")\n",
        "for entity, count in entity_counts.most_common(10):\n",
        "    print(f\"  {entity:30} {count:5}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zKL_6uL0T7z",
        "outputId": "d6b1969f-a633-48ad-c95d-75b23476532d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "STATISTICS\n",
            "====================================================================================================\n",
            "\n",
            "Total inscriptions: 138\n",
            "Total entities detected: 221\n",
            "\n",
            "Entities by label:\n",
            "  COGNOMEN                     87\n",
            "  DEDICATORY_FORMULA           63\n",
            "  NOMEN                        34\n",
            "  PRAENOMEN                    29\n",
            "  TRIBE                         6\n",
            "  EPITHET                       2\n",
            "\n",
            "Most common entities:\n",
            "  Dis Manibus                       21\n",
            "  Annorum                           16\n",
            "  Est                               10\n",
            "  Situs                              9\n",
            "  Hic                                8\n",
            "  Filio                              6\n",
            "  Galeria                            5\n",
            "  Iovi                               4\n",
            "  Caius                              4\n",
            "  Filius                             4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Save predictions to CSV (wide format)\n",
        "import pandas as pd\n",
        "\n",
        "# Define all possible labels\n",
        "LABELS = ['PRAENOMEN', 'NOMEN', 'COGNOMEN', 'TRIBE', 'EPITHET', 'DEDICATORY_FORMULA', 'ORIGIN']\n",
        "\n",
        "# Flatten predictions into wide format\n",
        "export_data = []\n",
        "for pred in predictions:\n",
        "    row = {\n",
        "        'id': pred['id'],\n",
        "        'leiden_source': pred['leiden_source'],\n",
        "        'text': pred['text'],\n",
        "    }\n",
        "\n",
        "    # Add a column for each label with entities found\n",
        "    for label in LABELS:\n",
        "        entities_with_label = [e['text'] for e in pred['entities'] if e['label'] == label]\n",
        "        row[label] = ' | '.join(entities_with_label) if entities_with_label else ''\n",
        "\n",
        "    # Add total entity count\n",
        "    row['entity_count'] = len(pred['entities'])\n",
        "\n",
        "    export_data.append(row)\n",
        "\n",
        "# Create dataframe\n",
        "export_df = pd.DataFrame(export_data)\n",
        "\n",
        "# Reorder columns: metadata first, then labels, then count\n",
        "column_order = ['id', 'leiden_source', 'text'] + LABELS + ['entity_count']\n",
        "export_df = export_df[column_order]\n",
        "\n",
        "# Save\n",
        "export_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "print(\"✅ Saved predictions to predictions.csv\\n\")\n",
        "print(f\"Dimensions: {export_df.shape[0]} rows × {export_df.shape[1]} columns\\n\")\n",
        "print(export_df.head(10).to_string())\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download('predictions.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "G5Itx2Xo0YmQ",
        "outputId": "34e2d153-db23-448f-f998-14d700d3ed0d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved predictions to predictions.csv\n",
            "\n",
            "Dimensions: 138 rows × 11 columns\n",
            "\n",
            "         id                                                                                                              leiden_source                                                                                     text PRAENOMEN    NOMEN         COGNOMEN TRIBE EPITHET DEDICATORY_FORMULA ORIGIN  entity_count\n",
            "0  actual_0                               Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]                   Iovi Optimo Maximo Quintus Cassius Cassianus Animo Libens Votum Posuit   Quintus  Cassius        Cassianus                             Iovi                    4\n",
            "1  actual_1        D(is) M(anibus) s(acrum) / Sycecale / v(ixit) an(n)o m(ensibus) [V] / soror[es] / Tricism[a] / Salcea / et Veget[a]      Dis Manibus Sacrum Sycecale Vixit Anno Mensibus V Sorores Tricisma Salcea et Vegeta                                                          Dis Manibus                    1\n",
            "2  actual_2                                                                                                    Fig(lina) Gem/ellian[a]                                                                       Figlina Gemelliana                                                                                         0\n",
            "3  actual_3  D(is) M(anibus) s(acrum) / Severus / Tongini / an(norum) XXI / h(ic) s(itus) e(st) s(it) t(ibi) / t(erra) l(evis) m(ater)  Dis Manibus Sacrum Severus Tongini Annorum XXI Hic Situs Est Sit Tibi Terra Levis Mater                                                          Dis Manibus                    1\n",
            "4  actual_4                                       Iovi O(ptimo) M(aximo) / C(aius) A[3]/s M[3]/s v(otum) [s(olvit) l(ibens?) a(nimo?)]                                 Iovi Optimo Maximo Caius As Ms Votum Solvit Libens Animo     Caius       As                                              Iovi                    3\n",
            "5  actual_5                             Turaesio / Turcau/di f(ilius?) et / Pans[3] / Maeilonis / f(ilii?) s(it) eis / t(erra) l(evis)                     Turaesio Turcaudi Filius et Pans Maeilonis Filii Sit Eis Terra Levis                             et Pans                                                     1\n",
            "6  actual_6                                                                                             Caeno / Loucin[i] / f(ilius) [                                                                     Caeno Loucini Filius                              Filius                                                     1\n",
            "7  actual_7                                                                                    Saturn/inus Bo/uti f(ilius) an(norum) [                                                          Saturninus Bouti Filius Annorum                             Annorum                                                     1\n",
            "8  actual_8                      an(norum)] / XVI h(ic) s(itus?) e(st) s(it) t(ibi) t(erra) / l(evis) Afelia avia / d(e) s(uo) f(ecit)                  Annorum XVI Hic Situs Est Sit Tibi Terra Levis Afelia Avia de Suo Fecit                                                                                         0\n",
            "9  actual_9                             C[3] / N[3] / Pap(iria?) [ann]or/um L h(ic) s(itus) e(st) / t(ibi) t(erra) l(evis) [3] / [3]V[                                   C N Papiria Annorum L Hic Situs Est Tibi Terra Levis V                     Annorum | Situs                                                     2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1c2b7620-bd60-4177-832d-e3834dec81d2\", \"predictions.csv\", 23478)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r inscription_model.zip training/model-best"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zYZrb4fQ8A4I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}