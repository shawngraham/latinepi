{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ok, used my latinepi (which needs cleaning up) to download *a lot* of inscriptions from edh. Then used a combination of scripts & llm to annotate them. In this notebook, we clean out the unsuccessfully annotated inscriptions, leaving us with a core of real data, and then we generate a whole bunch of synthetic inscriptions (thank god for formulaic epigraphy, eh?) that are correctly annotated, and mix them both together. The goal is to train the latinCy spaCy model to recognize the elements of funerary inscriptions, for data extraction from transcriptions. Why not?"
      ],
      "metadata": {
        "id": "rEmjAaQYVHdH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ufyr6JarIeFz"
      },
      "outputs": [],
      "source": [
        "!mkdir assets         # To store your raw data files (jsonl, csv)\n",
        "!mkdir configs        # To store configuration files\n",
        "!mkdir scripts        # To store helper scripts (like data conversion)\n",
        "!mkdir training       # To store the output of the training process\n",
        "!mkdir corpus         # To store the processed .spacy files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U spacy #already in colab\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#!pip install \"la-core-web-sm @ https://huggingface.co/latincy/la_core_web_sm/resolve/main/la_core_web_sm-any-py3-none-any.whl\"\n",
        "!pip install \"la-core-web-lg @ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl\"\n",
        "#\n",
        "# this is what we're going to retrain.\n",
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7rqCltXoIgt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5069244-2225-4303-b89c-be20ddfb2eb1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl\n",
            "  Downloading https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl (242.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy_lookups_data@ git+https://github.com/diyclassics/spacy-lookups-data.git#egg=spacy-lookups-data (from la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl)\n",
            "  Cloning https://github.com/diyclassics/spacy-lookups-data.git to /tmp/pip-install-h4uuczxr/spacy-lookups-data_b389a63dc42345b4a1cd7ea58638ef05\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/diyclassics/spacy-lookups-data.git /tmp/pip-install-h4uuczxr/spacy-lookups-data_b389a63dc42345b4a1cd7ea58638ef05\n",
            "  Resolved https://github.com/diyclassics/spacy-lookups-data.git to commit 5f2b7e60d3b461cd61649c0bb75f65a242b56ece\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<3.9.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy<3.9.0,>=3.8.3->la-core-web-lg@ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl) (2.0.1)\n",
            "Building wheels for collected packages: spacy_lookups_data\n",
            "  Building wheel for spacy_lookups_data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy_lookups_data: filename=spacy_lookups_data-1.0.5-py2.py3-none-any.whl size=100463154 sha256=505b2b787d688f9647e8cc773c7ca6c52bae10806cb1a05b25f7dba9d0a973bb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-llhnh2ts/wheels/51/65/b9/cc9ddb1b5331d1448963a943e3a9614f3b5e1f7c45844b4ae6\n",
            "Successfully built spacy_lookups_data\n",
            "Installing collected packages: spacy_lookups_data, la-core-web-lg\n",
            "Successfully installed la-core-web-lg-3.8.0 spacy_lookups_data-1.0.5\n",
            "Collecting spacy-transformers\n",
            "  Downloading spacy_transformers-1.3.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from spacy-transformers) (3.8.11)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy-transformers) (2.0.2)\n",
            "Collecting transformers<4.50.0,>=3.4.0 (from spacy-transformers)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from spacy-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from spacy-transformers) (2.5.2)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers)\n",
            "  Downloading spacy_alignments-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.1.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (3.5)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy-transformers) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<4.50.0,>=3.4.0->spacy-transformers)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers<4.50.0,>=3.4.0->spacy-transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy<4.1.0,>=3.5.0->spacy-transformers) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.1)\n",
            "Downloading spacy_transformers-1.3.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (795 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.8/795.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_alignments-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spacy-alignments, tokenizers, transformers, spacy-transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "Successfully installed spacy-alignments-0.9.2 spacy-transformers-1.3.9 tokenizers-0.21.4 transformers-4.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then you have to run this. It will say things have crashed. Ignore and continue.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "1vtGdT_tIlE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# don't use any of this, just generate some fresh ones below\n",
        "# start with some synthethic training annotations\n",
        "\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/2121b3ee828c4547fc2fd9470158e6d8/raw/ea0e2c578097ea67ab6c54e170d4559ea7c29790/simple-training.jsonl -O assets/synthetic-training.jsonl ## simple ones\n",
        "\n",
        "\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/f44663efc80916a75c736a38f024b371/raw/6585104793170cb5ef7c57dde16adf2d591dff04/synthetic-training.jsonl -O assets/synthetic-training.jsonl\n",
        "\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/f44663efc80916a75c736a38f024b371/raw/a924e157c8d87d13377dcc93e890879251d0c674/synthetic-training.jsonl # this is a mixture of 400+ lines of completely synthetic data and 1000+ lines of real data with llm generated annotations\n",
        "\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/f44663efc80916a75c736a38f024b371/raw/9b1d724d19b30ded168268af0fd959dccaae521e/synthetic-training.jsonl -O assets/synthethic-training.jsonl\n",
        "\n",
        "## and some synthetic testing data\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/3633224a209ab01f650f9dee9183888d/raw/9cc9dfeb566dc8465d744e6745af98af363a227c/testing-epigraphs-synthetic.csv -O assets/test-fake-epigraphs.csv\n",
        "\n",
        "## Real inscriptions from EDH\n",
        "#!wget https://gist.githubusercontent.com/shawngraham/8229265886e776624476331194c79934/raw/dac32c34cfb8528edaf1c2d961ed8b8f77e24c86/inscriptions.csv -O assets/inscriptions.csv"
      ],
      "metadata": {
        "id": "T3nrliM9Im0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean up some real inscriptions to add with the synthetic ones"
      ],
      "metadata": {
        "id": "reytWfT4SkkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### ok, try to clean up some real ones and then mix them in with the synthetic\n",
        "\n",
        "## this is a couple hundred rows of real inscriptions that were annotated\n",
        "## through combination of scripts & llm, but the results had issues with\n",
        "## annotation offsets\n",
        "!wget https://gist.githubusercontent.com/shawngraham/d949119d45f5cc661205a3bfbb266d86/raw/c93dfe9324d01478fe88f59f9cf9ee9d560dfc1e/annotations-to-clean-up-for-dataset.jsonl -O assets/to-clean.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-9PwnTCNCRf",
        "outputId": "cb23904b-61d4-4f09-fd35-6704024cf772"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-21 20:21:16--  https://gist.githubusercontent.com/shawngraham/d949119d45f5cc661205a3bfbb266d86/raw/c93dfe9324d01478fe88f59f9cf9ee9d560dfc1e/annotations-to-clean-up-for-dataset.jsonl\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 459000 (448K) [text/plain]\n",
            "Saving to: ‘assets/to-clean.jsonl’\n",
            "\n",
            "\rassets/to-clean.jso   0%[                    ]       0  --.-KB/s               \rassets/to-clean.jso 100%[===================>] 448.24K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-11-21 20:21:17 (15.6 MB/s) - ‘assets/to-clean.jsonl’ saved [459000/459000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## so we clean out the ones that are no good, just keeps the cleanest/easiest to fix\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "def clean_real_inscriptions(input_path, output_path, model='la_core_web_lg'):\n",
        "    \"\"\"\n",
        "    Cleans real inscription data:\n",
        "    1. Copies transcription → text\n",
        "    2. Re-validates all annotation spans\n",
        "    3. Drops records with errors or unalignable annotations\n",
        "    4. Outputs clean subset ready for training\n",
        "    \"\"\"\n",
        "    nlp = spacy.load(model)\n",
        "\n",
        "    stats = {\n",
        "        \"total\": 0,\n",
        "        \"has_error_flag\": 0,\n",
        "        \"no_transcription\": 0,\n",
        "        \"no_annotations\": 0,\n",
        "        \"perfect_match\": 0,\n",
        "        \"fixed_spans\": 0,\n",
        "        \"dropped_records\": 0,\n",
        "        \"saved\": 0\n",
        "    }\n",
        "\n",
        "    salvaged = []\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "                stats[\"total\"] += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Skip flagged errors\n",
        "            if record.get(\"_error\"):\n",
        "                stats[\"has_error_flag\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Use transcription as text\n",
        "            text = record.get('transcription', '').strip()\n",
        "            if not text:\n",
        "                stats[\"no_transcription\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Update record to use text field\n",
        "            record['text'] = text\n",
        "            record.pop('transcription', None)  # Remove redundant field\n",
        "\n",
        "            # Skip if no annotations\n",
        "            annotations = record.get('annotations', [])\n",
        "            if not isinstance(annotations, list) or not annotations:\n",
        "                stats[\"no_annotations\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Validate/fix annotations\n",
        "            doc = nlp.make_doc(text)\n",
        "            validated_ents = []\n",
        "            record_is_salvageable = True\n",
        "\n",
        "            for entity in annotations:\n",
        "                if not isinstance(entity, list) or len(entity) != 3:\n",
        "                    continue\n",
        "\n",
        "                start, end, label = entity\n",
        "\n",
        "                # Validate span is within text bounds\n",
        "                if start < 0 or end > len(text) or start >= end:\n",
        "                    record_is_salvageable = False\n",
        "                    break\n",
        "\n",
        "                # Check if span matches actual text\n",
        "                span_text = text[start:end]\n",
        "\n",
        "                # Try alignment\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "\n",
        "                if span is not None:\n",
        "                    # Span aligned successfully\n",
        "                    if span.start_char == start and span.end_char == end:\n",
        "                        # Perfect match\n",
        "                        validated_ents.append([start, end, label])\n",
        "                        stats[\"perfect_match\"] += 1\n",
        "                    else:\n",
        "                        # Adjusted but acceptable\n",
        "                        validated_ents.append([span.start_char, span.end_char, label])\n",
        "                        stats[\"fixed_spans\"] += 1\n",
        "                else:\n",
        "                    # Could not align - record is bad\n",
        "                    record_is_salvageable = False\n",
        "                    break\n",
        "\n",
        "            if record_is_salvageable and validated_ents:\n",
        "                record['annotations'] = validated_ents\n",
        "                salvaged.append(record)\n",
        "                stats[\"saved\"] += 1\n",
        "            else:\n",
        "                stats[\"dropped_records\"] += 1\n",
        "\n",
        "    # Save cleaned data\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for record in salvaged:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"\\n✅ Real inscription cleaning complete\")\n",
        "    print(f\"   Total records: {stats['total']}\")\n",
        "    print(f\"   Flagged as error: {stats['has_error_flag']}\")\n",
        "    print(f\"   No transcription: {stats['no_transcription']}\")\n",
        "    print(f\"   No annotations: {stats['no_annotations']}\")\n",
        "    print(f\"   Perfect annotations: {stats['perfect_match']}\")\n",
        "    print(f\"   Fixed spans: {stats['fixed_spans']}\")\n",
        "    print(f\"   Dropped (unrecoverable): {stats['dropped_records']}\")\n",
        "    print(f\"   ✅ SAVED: {stats['saved']} clean records\")\n",
        "    print(f\"   Saved to: {output_path}\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "# Run it\n",
        "clean_real_inscriptions('assets/to-clean.jsonl', 'assets/real_inscriptions_clean.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6Va11flNKvj",
        "outputId": "7da7a43c-9beb-459b-8815-99778685fa2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Real inscription cleaning complete\n",
            "   Total records: 1100\n",
            "   Flagged as error: 221\n",
            "   No transcription: 0\n",
            "   No annotations: 9\n",
            "   Perfect annotations: 2111\n",
            "   Fixed spans: 7932\n",
            "   Dropped (unrecoverable): 175\n",
            "   ✅ SAVED: 695 clean records\n",
            "   Saved to: assets/real_inscriptions_clean.jsonl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total': 1100,\n",
              " 'has_error_flag': 221,\n",
              " 'no_transcription': 0,\n",
              " 'no_annotations': 9,\n",
              " 'perfect_match': 2111,\n",
              " 'fixed_spans': 7932,\n",
              " 'dropped_records': 175,\n",
              " 'saved': 695}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## make some synthetic"
      ],
      "metadata": {
        "id": "bBRXbtgyTcG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Generate BOTH funerary and non-funerary Latin inscriptions.\n",
        "Uses weighted selection from real data.\n",
        "Saves to: assets/synthetic_focused.jsonl\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class FrequencyExtractor:\n",
        "    \"\"\"Extract frequency distributions from real inscription data\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_frequencies(input_path):\n",
        "        \"\"\"Extract frequencies from real data\"\"\"\n",
        "        praenomen_freq = Counter()\n",
        "        nomen_freq = Counter()\n",
        "        cognomen_freq = Counter()\n",
        "        tribe_freq = Counter()\n",
        "        origin_freq = Counter()\n",
        "        occupation_freq = Counter()\n",
        "        filiation_freq = Counter()\n",
        "        god_freq = Counter()\n",
        "\n",
        "        with open(input_path) as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                text = record.get('text', '')\n",
        "                annotations = record.get('annotations', [])\n",
        "\n",
        "                if not text or not isinstance(annotations, list):\n",
        "                    continue\n",
        "\n",
        "                for entity in annotations:\n",
        "                    if isinstance(entity, list) and len(entity) == 3:\n",
        "                        start, end, label = entity\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    if start < 0 or end > len(text):\n",
        "                        continue\n",
        "\n",
        "                    word = text[start:end].strip()\n",
        "\n",
        "                    if label == 'PRAENOMEN':\n",
        "                        praenomen_freq[word] += 1\n",
        "                    elif label == 'NOMEN':\n",
        "                        nomen_freq[word] += 1\n",
        "                    elif label == 'COGNOMEN':\n",
        "                        cognomen_freq[word] += 1\n",
        "                    elif label == 'TRIBE':\n",
        "                        tribe_freq[word] += 1\n",
        "                    elif label == 'ORIGIN':\n",
        "                        origin_freq[word] += 1\n",
        "                    elif label == 'OCCUPATION':\n",
        "                        occupation_freq[word] += 1\n",
        "                    elif label == 'FILIATION':\n",
        "                        filiation_freq[word] += 1\n",
        "\n",
        "        return (praenomen_freq, nomen_freq, cognomen_freq, tribe_freq,\n",
        "                origin_freq, occupation_freq, filiation_freq)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_weighted_list(freq_counter, top_n=30):\n",
        "        \"\"\"Convert frequency counter to weighted selection list\"\"\"\n",
        "        items = [word for word, _ in freq_counter.most_common(top_n)]\n",
        "        return items if items else []\n",
        "\n",
        "\n",
        "class MixedInscriptionGenerator:\n",
        "    \"\"\"Generate both funerary and non-funerary inscriptions\"\"\"\n",
        "\n",
        "    def __init__(self, praenomina, nomina, cognomina, tribes, origins, occupations=None, filiations=None):\n",
        "        self.PRAENOMINA = praenomina or ['M', 'L', 'C', 'T', 'Q']\n",
        "        self.NOMINA = nomina or ['IVLIVS', 'CLAVDIVS', 'FLAVIVS']\n",
        "        self.COGNOMINA = cognomina or ['VICTOR', 'FELIX', 'MAXIMVS']\n",
        "        self.TRIBES = tribes or ['GAL', 'POL', 'VEL']\n",
        "        self.ORIGINS = origins or ['Roma', 'Italia', 'Hispania', 'Gallia', 'Britannia']\n",
        "\n",
        "        self.OCCUPATIONS = occupations or [\n",
        "            'miles', 'centurio', 'optio', 'tribunus', 'praefectus',\n",
        "            'beneficiarius', 'librarius', 'aquilifer', 'veteranus'\n",
        "        ]\n",
        "\n",
        "        self.FILIATIONS = filiations or [\n",
        "            'filius', 'filiae', 'filia', 'patri', 'matri', 'coniugi',\n",
        "            'fratri', 'sorori', 'libertus', 'liberta'\n",
        "        ]\n",
        "\n",
        "        self.STATUS = ['liber', 'libertus', 'servus']\n",
        "\n",
        "        self.MILITARY_UNITS = [\n",
        "            'legionis I', 'legionis II', 'legionis VII', 'legionis X',\n",
        "            'legionis XIII', 'cohortis I', 'alae I'\n",
        "        ]\n",
        "\n",
        "        self.EPITHETS = ['BENE MERENTI', 'PIISSIMO', 'DVLCISSIMO', 'CARISSIMO', 'OPTIMO']\n",
        "\n",
        "        self.GODS = [\n",
        "            'Iovi Optimo Maximo', 'Minervae', 'Marti', 'Veneri', 'Mercurio',\n",
        "            'Neptvno', 'Dianae', 'Silvano'\n",
        "        ]\n",
        "\n",
        "        self.DATIVE_MAP = {\n",
        "            'IVLIVS': 'IVLIO', 'CLAVDIVS': 'CLAVDIO', 'FLAVIVS': 'FLAVIO',\n",
        "            'VICTOR': 'VICTORI', 'FELIX': 'FELICI', 'MAXIMVS': 'MAXIMO',\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_roman(num):\n",
        "        roman_map = [(100, 'C'), (90, 'XC'), (50, 'L'), (40, 'XL'), (10, 'X'),\n",
        "                     (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')]\n",
        "        result = ''\n",
        "        for val, sym in roman_map:\n",
        "            count = num // val\n",
        "            result += sym * count\n",
        "            num -= val * count\n",
        "        return result\n",
        "\n",
        "    def _to_dative(self, latin_word):\n",
        "        return self.DATIVE_MAP.get(latin_word, latin_word)\n",
        "\n",
        "    def _generate_funerary(self):\n",
        "        \"\"\"Generate a single funerary inscription\"\"\"\n",
        "        spans = []\n",
        "        text_parts = []\n",
        "\n",
        "        if random.random() < 0.8:\n",
        "            start = 0\n",
        "            text_parts.append(\"D M\")\n",
        "            spans.append([start, 3, \"DEDICATORY_FORMULA\"])\n",
        "\n",
        "        prae_dec = random.choice(self.PRAENOMINA)\n",
        "        nomen_dec = random.choice(self.NOMINA)\n",
        "        cog_dec = random.choice(self.COGNOMINA)\n",
        "\n",
        "        start = len(\" \".join(text_parts)) + 1 if text_parts else 0\n",
        "        text_parts.append(prae_dec)\n",
        "        spans.append([start, start + len(prae_dec), \"PRAENOMEN\"])\n",
        "\n",
        "        nomen_dat = self._to_dative(nomen_dec)\n",
        "        start = len(\" \".join(text_parts)) + 1\n",
        "        text_parts.append(nomen_dat)\n",
        "        spans.append([start, start + len(nomen_dat), \"NOMEN\"])\n",
        "\n",
        "        if random.random() < 0.25:\n",
        "            tribe = random.choice(self.TRIBES)\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(tribe)\n",
        "            spans.append([start, start + len(tribe), \"TRIBE\"])\n",
        "\n",
        "        if random.random() < 0.9:\n",
        "            cog_dat = self._to_dative(cog_dec)\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(cog_dat)\n",
        "            spans.append([start, start + len(cog_dat), \"COGNOMEN\"])\n",
        "\n",
        "        if random.random() < 0.65:\n",
        "            years = random.randint(15, 90)\n",
        "            roman_num = self._to_roman(years)\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(\"vixit\")\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            age_str = f\"ann(is) {roman_num}\"\n",
        "            text_parts.append(age_str)\n",
        "            spans.append([start, start + len(age_str), \"AGE\"])\n",
        "\n",
        "        if random.random() < 0.3:\n",
        "            occ = random.choice(self.OCCUPATIONS)\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(occ)\n",
        "            spans.append([start, start + len(occ), \"OCCUPATION\"])\n",
        "\n",
        "            if random.random() < 0.6:\n",
        "                unit = random.choice(self.MILITARY_UNITS)\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(unit)\n",
        "                spans.append([start, start + len(unit), \"MILITARY_UNIT\"])\n",
        "\n",
        "        if random.random() < 0.6:\n",
        "            ded_prae = random.choice(self.PRAENOMINA)\n",
        "            ded_nomen = random.choice(self.NOMINA)\n",
        "            ded_cog = random.choice(self.COGNOMINA)\n",
        "\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(ded_prae)\n",
        "            spans.append([start, start + len(ded_prae), \"PRAENOMEN\"])\n",
        "\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(ded_nomen)\n",
        "            spans.append([start, start + len(ded_nomen), \"NOMEN\"])\n",
        "\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(ded_cog)\n",
        "            spans.append([start, start + len(ded_cog), \"COGNOMEN\"])\n",
        "\n",
        "            if random.random() < 0.7:\n",
        "                filiation = random.choice(self.FILIATIONS)\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(filiation)\n",
        "                spans.append([start, start + len(filiation), \"FILIATION\"])\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                epithet = random.choice(self.EPITHETS)\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(epithet)\n",
        "                spans.append([start, start + len(epithet), \"EPITHET\"])\n",
        "\n",
        "            if random.random() < 0.6:\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(random.choice(['posuit', 'fecit', 'dedicavit']))\n",
        "\n",
        "        full_text = \" \".join(text_parts)\n",
        "        return {\n",
        "            \"id\": f\"syn_{random.randint(100000, 999999)}\",\n",
        "            \"type\": \"FUNERARY\",\n",
        "            \"text\": full_text,\n",
        "            \"annotations\": spans\n",
        "        }\n",
        "\n",
        "    def _generate_non_funerary(self):\n",
        "        \"\"\"Generate a votive/dedicatory inscription\"\"\"\n",
        "        spans = []\n",
        "        text_parts = []\n",
        "\n",
        "        god = random.choice(self.GODS)\n",
        "        start = 0\n",
        "        text_parts.append(god)\n",
        "        spans.append([start, len(god), \"DEDICATORY_FORMULA\"])\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            prae = random.choice(self.PRAENOMINA)\n",
        "            nomen = random.choice(self.NOMINA)\n",
        "            cog = random.choice(self.COGNOMINA)\n",
        "\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(prae)\n",
        "            spans.append([start, start + len(prae), \"PRAENOMEN\"])\n",
        "\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(nomen)\n",
        "            spans.append([start, start + len(nomen), \"NOMEN\"])\n",
        "\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(cog)\n",
        "            spans.append([start, start + len(cog), \"COGNOMEN\"])\n",
        "\n",
        "            if random.random() < 0.3:\n",
        "                tribe = random.choice(self.TRIBES)\n",
        "                start = len(\" \".join(text_parts)) + 1\n",
        "                text_parts.append(tribe)\n",
        "                spans.append([start, start + len(tribe), \"TRIBE\"])\n",
        "\n",
        "        if random.random() < 0.3:\n",
        "            origin = random.choice(self.ORIGINS)\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(f\"ex {origin}\")\n",
        "            spans.append([start + 3, start + 3 + len(origin), \"ORIGIN\"])\n",
        "\n",
        "        if random.random() < 0.7:\n",
        "            start = len(\" \".join(text_parts)) + 1\n",
        "            text_parts.append(random.choice(['posuit', 'vovit', 'fecit']))\n",
        "\n",
        "        full_text = \" \".join(text_parts)\n",
        "        return {\n",
        "            \"id\": f\"syn_{random.randint(100000, 999999)}\",\n",
        "            \"type\": \"NON_FUNERARY\",\n",
        "            \"text\": full_text,\n",
        "            \"annotations\": spans\n",
        "        }\n",
        "\n",
        "    def generate_inscriptions(self, count=5000, funerary_ratio=0.7):\n",
        "        \"\"\"Generate mix of funerary and non-funerary\"\"\"\n",
        "        data = []\n",
        "\n",
        "        num_funerary = int(count * funerary_ratio)\n",
        "        num_votive = count - num_funerary\n",
        "\n",
        "        for _ in range(num_funerary):\n",
        "            data.append(json.dumps(self._generate_funerary()))\n",
        "\n",
        "        for _ in range(num_votive):\n",
        "            data.append(json.dumps(self._generate_non_funerary()))\n",
        "\n",
        "        random.shuffle(data)\n",
        "        return data\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Extracting frequencies from real inscriptions...\")\n",
        "    praenomina_freq, nomina_freq, cognomina_freq, tribe_freq, origin_freq, occupation_freq, filiation_freq = \\\n",
        "        FrequencyExtractor.extract_frequencies('assets/real_inscriptions_clean.jsonl')\n",
        "\n",
        "    praenomina = FrequencyExtractor.get_weighted_list(praenomina_freq, top_n=20)\n",
        "    nomina = FrequencyExtractor.get_weighted_list(nomina_freq, top_n=30)\n",
        "    cognomina = FrequencyExtractor.get_weighted_list(cognomina_freq, top_n=40)\n",
        "    tribes = FrequencyExtractor.get_weighted_list(tribe_freq, top_n=15)\n",
        "    origins = FrequencyExtractor.get_weighted_list(origin_freq, top_n=20)\n",
        "    occupations = FrequencyExtractor.get_weighted_list(occupation_freq, top_n=20)\n",
        "    filiations = FrequencyExtractor.get_weighted_list(filiation_freq, top_n=15)\n",
        "\n",
        "    print(f\"✅ Extracted frequencies\")\n",
        "    print(f\"Generating inscriptions...\")\n",
        "\n",
        "    gen = MixedInscriptionGenerator(praenomina, nomina, cognomina, tribes, origins, occupations, filiations)\n",
        "    inscriptions = gen.generate_inscriptions(count=5000, funerary_ratio=0.7)\n",
        "\n",
        "    with open('assets/synthetic_focused.jsonl', 'w') as f:\n",
        "        for inscription in inscriptions:\n",
        "            f.write(inscription + '\\n')\n",
        "\n",
        "    print(f\"✅ Saved {len(inscriptions)} inscriptions to assets/synthetic_focused.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjPUNOzHLpWf",
        "outputId": "f962b06b-a18e-4c02-c921-4be0a432e086"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting frequencies from real inscriptions...\n",
            "✅ Extracted frequencies\n",
            "Generating inscriptions...\n",
            "✅ Saved 5000 inscriptions to assets/synthetic_focused.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## combine real & synthethic"
      ],
      "metadata": {
        "id": "QrrC2vGtS63j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Harmonize real and synthetic data to support both funerary and non-funerary schemas.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "REAL_LABEL_MAPPING = {\n",
        "    \"PRAENOMEN\": \"PRAENOMEN\",\n",
        "    \"NOMEN\": \"NOMEN\",\n",
        "    \"COGNOMEN\": \"COGNOMEN\",\n",
        "    \"TRIBE\": \"TRIBE\",\n",
        "    \"EPITHET\": \"EPITHET\",\n",
        "    \"ORIGIN\": \"ORIGIN\",\n",
        "    \"OCCUPATION\": \"OCCUPATION\",\n",
        "    \"MILITARY_UNIT\": \"MILITARY_UNIT\",\n",
        "    \"AGE\": \"AGE\",\n",
        "    \"FILIATION\": \"FILIATION\",\n",
        "    \"FUNERARY_FORMULA\": \"DEDICATORY_FORMULA\",\n",
        "    \"DEDICATION_TO_THE_GODS\": \"DEDICATORY_FORMULA\",\n",
        "    \"BENE_MERENTI\": \"EPITHET\",\n",
        "    # Drop these\n",
        "    \"RELATIONSHIP\": None,\n",
        "    \"AGE_YEARS\": None,\n",
        "    \"AGE_DAYS\": None,\n",
        "    \"AGE_MONTHS\": None,\n",
        "    \"ACTION\": None,\n",
        "    \"VERB\": None,\n",
        "}\n",
        "\n",
        "\n",
        "def harmonize_real_data(input_path, output_path):\n",
        "    \"\"\"Harmonize real inscriptions, preserve type if available\"\"\"\n",
        "    harmonized = []\n",
        "    dropped = 0\n",
        "\n",
        "    with open(input_path) as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            text = record.get('text', '')\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            annotations = record.get('annotations', [])\n",
        "            if not isinstance(annotations, list):\n",
        "                continue\n",
        "\n",
        "            new_annotations = []\n",
        "            for entity in annotations:\n",
        "                if not isinstance(entity, list) or len(entity) != 3:\n",
        "                    continue\n",
        "\n",
        "                start, end, label = entity\n",
        "                new_label = REAL_LABEL_MAPPING.get(label)\n",
        "\n",
        "                if new_label is None:\n",
        "                    dropped += 1\n",
        "                    continue\n",
        "\n",
        "                if start < 0 or end > len(text) or start >= end:\n",
        "                    continue\n",
        "\n",
        "                new_annotations.append([start, end, new_label])\n",
        "\n",
        "            if new_annotations:\n",
        "                record['annotations'] = new_annotations\n",
        "                # Default to FUNERARY if type not specified\n",
        "                if 'type' not in record:\n",
        "                    record['type'] = 'FUNERARY'\n",
        "                harmonized.append(record)\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        for record in harmonized:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"✅ Harmonized real inscriptions\")\n",
        "    print(f\"   Records saved: {len(harmonized)}\")\n",
        "    print(f\"   Annotations dropped: {dropped}\")\n",
        "\n",
        "\n",
        "def combine_and_split(real_path, synthetic_path, train_out, dev_out):\n",
        "    \"\"\"Combine real + synthetic and split into train/dev\"\"\"\n",
        "    real_records = []\n",
        "    with open(real_path) as f:\n",
        "        real_records = [json.loads(line) for line in f]\n",
        "\n",
        "    synthetic_records = []\n",
        "    with open(synthetic_path) as f:\n",
        "        synthetic_records = [json.loads(line) for line in f]\n",
        "\n",
        "    combined = real_records + synthetic_records\n",
        "\n",
        "    print(f\"\\n✅ Combined data:\")\n",
        "    print(f\"   Real: {len(real_records)} ({len(real_records)/len(combined)*100:.1f}%)\")\n",
        "    print(f\"   Synthetic: {len(synthetic_records)} ({len(synthetic_records)/len(combined)*100:.1f}%)\")\n",
        "    print(f\"   Total: {len(combined)}\")\n",
        "\n",
        "    train, dev = train_test_split(combined, test_size=0.2, random_state=42)\n",
        "\n",
        "    with open(train_out, 'w') as f:\n",
        "        for record in train:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    with open(dev_out, 'w') as f:\n",
        "        for record in dev:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"\\n✅ Split into train/dev:\")\n",
        "    print(f\"   Train: {len(train)}\")\n",
        "    print(f\"   Dev: {len(dev)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== HARMONIZING REAL DATA ===\")\n",
        "    harmonize_real_data('assets/real_inscriptions_clean.jsonl',\n",
        "                       'assets/real_focused.jsonl')\n",
        "\n",
        "    print(\"\\n=== COMBINING AND SPLITTING ===\")\n",
        "    combine_and_split('assets/real_focused.jsonl',\n",
        "                     'assets/synthetic_focused.jsonl',\n",
        "                     'assets/train_focused.jsonl',\n",
        "                     'assets/dev_focused.jsonl')\n",
        "\n",
        "    print(\"\\n✅ Ready for alignment and training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiWNnvEYSHwr",
        "outputId": "7b3889c6-0fcc-4905-8dd1-2d94af1472bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== HARMONIZING REAL DATA ===\n",
            "✅ Harmonized real inscriptions\n",
            "   Records saved: 650\n",
            "   Annotations dropped: 3046\n",
            "\n",
            "=== COMBINING AND SPLITTING ===\n",
            "\n",
            "✅ Combined data:\n",
            "   Real: 650 (11.5%)\n",
            "   Synthetic: 5000 (88.5%)\n",
            "   Total: 5650\n",
            "\n",
            "✅ Split into train/dev:\n",
            "   Train: 4520\n",
            "   Dev: 1130\n",
            "\n",
            "✅ Ready for alignment and training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Align and convert to .spacy\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "def align_annotations(input_path, output_path):\n",
        "    nlp = spacy.load('la_core_web_lg')\n",
        "    corrected_records = []\n",
        "\n",
        "    stats = {\"total\": 0, \"perfect\": 0, \"fixed\": 0, \"malformed\": 0, \"dropped\": 0}\n",
        "\n",
        "    with open(input_path, 'r') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "                stats[\"total\"] += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            text = record.get('text', '')\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            annotations = record.get('annotations', [])\n",
        "            doc = nlp.make_doc(text)\n",
        "            corrected_ents = []\n",
        "\n",
        "            for entity in annotations:\n",
        "                if isinstance(entity, list) and len(entity) == 3:\n",
        "                    start, end, label = entity\n",
        "                else:\n",
        "                    stats[\"malformed\"] += 1\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    start, end = int(start), int(end)\n",
        "                except (ValueError, TypeError):\n",
        "                    stats[\"malformed\"] += 1\n",
        "                    continue\n",
        "\n",
        "                if start < 0 or end > len(text) or start >= end:\n",
        "                    stats[\"malformed\"] += 1\n",
        "                    continue\n",
        "\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "\n",
        "                if span is not None:\n",
        "                    if span.start_char == start and span.end_char == end:\n",
        "                        stats[\"perfect\"] += 1\n",
        "                    else:\n",
        "                        stats[\"fixed\"] += 1\n",
        "                    corrected_ents.append([span.start_char, span.end_char, label])\n",
        "                else:\n",
        "                    stats[\"dropped\"] += 1\n",
        "\n",
        "            record['annotations'] = corrected_ents\n",
        "            corrected_records.append(record)\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        for record in corrected_records:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"\\n✅ Aligned {input_path}\")\n",
        "    print(f\"   Perfect: {stats['perfect']}, Fixed: {stats['fixed']}, Dropped: {stats['dropped']}\")\n",
        "\n",
        "# Align both\n",
        "align_annotations('assets/train_focused.jsonl', 'assets/train_focused_aligned.jsonl')\n",
        "align_annotations('assets/dev_focused.jsonl', 'assets/dev_focused_aligned.jsonl')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enXTp35tNSr5",
        "outputId": "bb93c89a-2b70-42bc-96c3-3f59476a79ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Aligned assets/train_focused.jsonl\n",
            "   Perfect: 28387, Fixed: 231, Dropped: 41\n",
            "\n",
            "✅ Aligned assets/dev_focused.jsonl\n",
            "   Perfect: 7089, Fixed: 41, Dropped: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Convert to .spacy format\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "def create_spacy_file(input_path, output_path, model='la_core_web_lg'):\n",
        "    nlp = spacy.load(model)\n",
        "    db = DocBin()\n",
        "\n",
        "    with open(input_path) as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line)\n",
        "            text = record.get('text', '')\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            doc = nlp.make_doc(text)\n",
        "            ents = []\n",
        "\n",
        "            for start, end, label in record.get('annotations', []):\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "                if span:\n",
        "                    ents.append(span)\n",
        "\n",
        "            doc.ents = filter_spans(ents)\n",
        "            db.add(doc)\n",
        "\n",
        "    db.to_disk(output_path)\n",
        "    print(f\"✅ Created {output_path}\")\n",
        "\n",
        "create_spacy_file('assets/train_focused_aligned.jsonl', 'corpus/train.spacy')\n",
        "create_spacy_file('assets/dev_focused_aligned.jsonl', 'corpus/dev.spacy')\n",
        "\n",
        "print(\"\\n✅ Ready for training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEuAI4amusm5",
        "outputId": "5833e60e-8884-4fc4-d7d6-69b087c5b8e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created corpus/train.spacy\n",
            "✅ Created corpus/dev.spacy\n",
            "\n",
            "✅ Ready for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Check sample output\n",
        "with open('assets/synthetic_focused.jsonl') as f:\n",
        "    samples = [json.loads(f.readline()) for _ in range(3)]\n",
        "    print(\"\\nSample synthetic inscriptions:\")\n",
        "    for sample in samples:\n",
        "        print(f\"\\n  {sample['text']}\")\n",
        "        labels = [ann[2] for ann in sample['annotations']]\n",
        "        print(f\"  Labels: {set(labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEK1LZWuuu6e",
        "outputId": "79b6db7d-e0d1-49fb-faae-43168df533fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample synthetic inscriptions:\n",
            "\n",
            "  D M M--- Iulius Florus vixit ann(is) XLIX rei legionis I\n",
            "  Labels: {'COGNOMEN', 'OCCUPATION', 'AGE', 'NOMEN', 'DEDICATORY_FORMULA', 'MILITARY_UNIT', 'PRAENOMEN'}\n",
            "\n",
            "  D M Publi Quinti Felicis vixit ann(is) LXXVII\n",
            "  Labels: {'COGNOMEN', 'AGE', 'NOMEN', 'DEDICATORY_FORMULA', 'PRAENOMEN'}\n",
            "\n",
            "  D M Caius Caecilius Secundo Imperator Cai Sulpicio Coloni Sexti BENE MERENTI\n",
            "  Labels: {'COGNOMEN', 'OCCUPATION', 'NOMEN', 'DEDICATORY_FORMULA', 'FILIATION', 'EPITHET', 'PRAENOMEN'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Generate the base config ---\n",
        "!python -m spacy init config configs/config.cfg --lang la --pipeline tok2vec,ner --optimize accuracy --force\n",
        "\n",
        "print(\"✅ Base 'config.cfg' generated.\")\n",
        "\n",
        "# --- 2. Load and Modify ---\n",
        "config_path = Path(\"configs/config.cfg\")\n",
        "config = spacy.util.load_config(config_path)\n",
        "\n",
        "# Define the model we are using\n",
        "LATIN_MODEL = \"la_core_web_lg\"\n",
        "\n",
        "# --- Part A: Initialize Vectors (CRITICAL FOR LG MODELS) ---\n",
        "# This loads the 300-dim vectors into the vocab so the tok2vec layer can find them.\n",
        "config[\"initialize\"][\"vectors\"] = LATIN_MODEL\n",
        "\n",
        "# --- Part B: Source the tok2vec component ---\n",
        "config[\"components\"][\"tok2vec\"] = {\n",
        "    \"source\": LATIN_MODEL,\n",
        "    \"component\": \"tok2vec\"\n",
        "}\n",
        "\n",
        "# --- Part C: Connect NER to the vectors ---\n",
        "# ERROR CORRECTION: The tok2vec OUTPUT width is 96, even if the input vectors are 300.\n",
        "config[\"components\"][\"ner\"][\"model\"][\"tok2vec\"] = {\n",
        "    \"@architectures\": \"spacy.Tok2VecListener.v1\",\n",
        "    \"width\": 96,  # <--- Reverted to 96. This matches the output of la_core_web_lg.\n",
        "    \"upstream\": \"tok2vec\"\n",
        "}\n",
        "\n",
        "config[\"nlp\"][\"batch_size\"] = 200\n",
        "\n",
        "# --- Part D: Paths and Freezing ---\n",
        "config[\"paths\"][\"train\"] = \"./corpus/train.spacy\"\n",
        "config[\"paths\"][\"dev\"] = \"./corpus/dev.spacy\"\n",
        "\n",
        "# Freeze tok2vec so we don't ruin the pretrained Latin intelligence\n",
        "#config[\"training\"][\"frozen_components\"] = [\"tok2vec\"]\n",
        "# or unfreeze it, see what happens\n",
        "config[\"training\"][\"frozen_components\"] = []\n",
        "#config[\"training\"][\"max_epochs\"]= 100\n",
        "config[\"training\"][\"max_epochs\"] = 2\n",
        "# Mark it as annotating so it actually runs\n",
        "config[\"training\"][\"annotating_components\"] = [\"tok2vec\"]\n",
        "\n",
        "# --- 3. Save ---\n",
        "config.to_disk(config_path)\n",
        "\n",
        "print(f\"✅ Config updated for {LATIN_MODEL}. Listener width set to 96 (correct output dim).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mKD51UtI_uY",
        "outputId": "d762901a-04e1-4de3-b3af-4ed3a124f1bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: la\n",
            "- Pipeline: ner\n",
            "- Optimize for: accuracy\n",
            "- Hardware: CPU\n",
            "- Transformer: None\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "configs/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "✅ Base 'config.cfg' generated.\n",
            "✅ Config updated for la_core_web_lg. Listener width set to 96 (correct output dim).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training process!\n",
        "!python -m spacy train configs/config.cfg --output ./training/ #--gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXBY8quJEL5",
        "outputId": "d327176a-1085-49c5-d77f-b8b3b783697a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: training\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Set annotations on update for: ['tok2vec']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     86.87   16.07   17.20   15.08    0.16\n",
            "  0     200        245.54   5414.30   91.56   92.95   90.21    0.92\n",
            "  1     400        260.20   2632.45   93.02   95.72   90.47    0.93\n",
            "  1     600        333.50   3028.87   92.66   94.36   91.02    0.93\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.scorer import Scorer\n",
        "from spacy.training import Example\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "def evaluate_final(model_path, dev_data_path):\n",
        "    print(f\"--- Evaluating {model_path} ---\")\n",
        "    nlp = spacy.load(model_path)\n",
        "    db = DocBin().from_disk(dev_data_path)\n",
        "    docs = list(db.get_docs(nlp.vocab))\n",
        "\n",
        "    examples = []\n",
        "    for doc in docs:\n",
        "        examples.append(Example(nlp(doc.text), doc))\n",
        "\n",
        "    scores = Scorer().score(examples)\n",
        "\n",
        "    print(f\"{'LABEL':<30} {'PREC':<8} {'REC':<8} {'F1':<8}\")\n",
        "    print(\"-\" * 60)\n",
        "    for label, metrics in scores['ents_per_type'].items():\n",
        "        print(f\"{label:<30} {metrics['p']:.2f}     {metrics['r']:.2f}     {metrics['f']:.2f}\")\n",
        "\n",
        "evaluate_final(\"training/model-best\", \"corpus/dev.spacy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11NHuAmJJKl4",
        "outputId": "d9004d0b-a07e-42dc-ae29-620270c7ece2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating training/model-best ---\n",
            "LABEL                          PREC     REC      F1      \n",
            "------------------------------------------------------------\n",
            "DEDICATORY_FORMULA             0.97     0.79     0.87\n",
            "PRAENOMEN                      0.97     0.98     0.97\n",
            "NOMEN                          0.96     0.94     0.95\n",
            "COGNOMEN                       0.95     0.92     0.93\n",
            "FILIATION                      0.91     0.91     0.91\n",
            "TRIBE                          0.96     0.94     0.95\n",
            "AGE                            0.99     1.00     0.99\n",
            "OCCUPATION                     0.85     0.78     0.81\n",
            "MILITARY_UNIT                  0.89     0.77     0.82\n",
            "EPITHET                        0.96     0.81     0.88\n",
            "ORIGIN                         1.00     1.00     1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL: Per-label evaluation\n",
        "from spacy.training import Example\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "nlp = spacy.load(\"training/model-best\")\n",
        "\n",
        "# Load dev.spacy\n",
        "dev_docs = list(DocBin().from_disk(\"corpus/dev.spacy\").get_docs(nlp.vocab))\n",
        "\n",
        "# Score by label\n",
        "from collections import defaultdict\n",
        "scores = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
        "\n",
        "for doc in dev_docs:\n",
        "    gold_ents = {(e.start_char, e.end_char, e.label_) for e in doc.ents}\n",
        "    pred_ents = {(e.start_char, e.end_char, e.label_) for e in nlp(doc.text).ents}\n",
        "\n",
        "    for label in set([e[2] for e in gold_ents | pred_ents]):\n",
        "        gold_with_label = {e for e in gold_ents if e[2] == label}\n",
        "        pred_with_label = {e for e in pred_ents if e[2] == label}\n",
        "\n",
        "        scores[label][\"tp\"] += len(gold_with_label & pred_with_label)\n",
        "        scores[label][\"fp\"] += len(pred_with_label - gold_with_label)\n",
        "        scores[label][\"fn\"] += len(gold_with_label - pred_with_label)\n",
        "\n",
        "print(\"\\nPer-label F1 on DEV set:\")\n",
        "print(f\"{'LABEL':<25} {'PREC':<8} {'REC':<8} {'F1':<8}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for label in sorted(scores.keys()):\n",
        "    tp = scores[label][\"tp\"]\n",
        "    fp = scores[label][\"fp\"]\n",
        "    fn = scores[label][\"fn\"]\n",
        "\n",
        "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
        "\n",
        "    print(f\"{label:<25} {prec:.2f}     {rec:.2f}     {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gunwmI3-GJQM",
        "outputId": "acddc1f2-3e5e-4178-9c02-276dc770d8b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-label F1 on DEV set:\n",
            "LABEL                     PREC     REC      F1      \n",
            "--------------------------------------------------\n",
            "AGE                       0.99     1.00     0.99\n",
            "COGNOMEN                  0.95     0.92     0.93\n",
            "DEDICATORY_FORMULA        0.97     0.79     0.87\n",
            "EPITHET                   0.96     0.81     0.88\n",
            "FILIATION                 0.91     0.91     0.91\n",
            "MILITARY_UNIT             0.89     0.77     0.82\n",
            "NOMEN                     0.96     0.94     0.95\n",
            "OCCUPATION                0.85     0.78     0.81\n",
            "ORIGIN                    1.00     1.00     1.00\n",
            "PRAENOMEN                 0.97     0.98     0.97\n",
            "TRIBE                     0.96     0.94     0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ok, deploy the model on real data"
      ],
      "metadata": {
        "id": "Jc8MHpKXGNnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import spacy\n",
        "\n",
        "# The best model should be saved as model-best during training\n",
        "# Load it\n",
        "nlp = spacy.load(\"training/model-best\")\n",
        "\n",
        "# Test on real inscriptions from your dataset\n",
        "test_cases = [\n",
        "    \"Dis Manibus Lucio Ocratio Corrintho vixit annos XXX dies XI Ocratia Silvana filio piissimo bene merenti fecit\",\n",
        "    \"Caius Pompeius Caius libertus librarius\",\n",
        "    \"Dis Manibus sacrum Fortunatus municipum municipii Ipolcobulensiorum servus annorum XXXXIII pius\",\n",
        "    \"Dis Manibus Spediae Luci filiae Severae coniugi Luci Valeri Montani Quinti fili primi pili legionis XIII\",\n",
        "    \"Figlinae Ocreana\"\n",
        "]\n",
        "\n",
        "print(\"Testing on real inscriptions:\\n\")\n",
        "for text in test_cases:\n",
        "    doc = nlp(text)\n",
        "    print(f\"Text: {text[:70]}...\")\n",
        "    print(\"Entities predicted:\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"  {ent.text:25} → {ent.label_:15}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQiq0hK9V8ip",
        "outputId": "591db0d0-9131-459a-e582-de029787fc90"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on real inscriptions:\n",
            "\n",
            "Text: Dis Manibus Lucio Ocratio Corrintho vixit annos XXX dies XI Ocratia Si...\n",
            "Entities predicted:\n",
            "  Dis Manibus               → DEDICATORY_FORMULA\n",
            "  Lucio                     → NOMEN          \n",
            "  Ocratio                   → NOMEN          \n",
            "  Corrintho                 → COGNOMEN       \n",
            "  Ocratia                   → TRIBE          \n",
            "\n",
            "Text: Caius Pompeius Caius libertus librarius...\n",
            "Entities predicted:\n",
            "  Caius                     → PRAENOMEN      \n",
            "  Pompeius                  → NOMEN          \n",
            "\n",
            "Text: Dis Manibus sacrum Fortunatus municipum municipii Ipolcobulensiorum se...\n",
            "Entities predicted:\n",
            "  Dis Manibus               → DEDICATORY_FORMULA\n",
            "\n",
            "Text: Dis Manibus Spediae Luci filiae Severae coniugi Luci Valeri Montani Qu...\n",
            "Entities predicted:\n",
            "  Dis Manibus               → DEDICATORY_FORMULA\n",
            "  Spediae                   → NOMEN          \n",
            "  Luci                      → FILIATION      \n",
            "  filiae                    → NOMEN          \n",
            "  Luci                      → PRAENOMEN      \n",
            "  Valeri                    → NOMEN          \n",
            "  Montani                   → COGNOMEN       \n",
            "  Quinti                    → FILIATION      \n",
            "  fili                      → COGNOMEN       \n",
            "  legionis XIII             → MILITARY_UNIT  \n",
            "\n",
            "Text: Figlinae Ocreana...\n",
            "Entities predicted:\n",
            "  Figlinae                  → DEDICATORY_FORMULA\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real Data Time\n",
        "\n",
        "Loading in data from P."
      ],
      "metadata": {
        "id": "b3bp9U-xyfDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Process Leiden conventions\n",
        "import csv\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "class LeidenProcessor:\n",
        "    \"\"\"Convert Leiden conventions to clean transcription\"\"\"\n",
        "\n",
        "    # Map abbreviations to their expansions\n",
        "    # Capitalize proper nouns, keep others lowercase\n",
        "    ABBREV_PROPER = {  # Names (capitalize)\n",
        "        'Q': 'Quintus', 'C': 'Caius', 'M': 'Marcus', 'L': 'Lucius',\n",
        "        'T': 'Titus', 'P': 'Publius', 'D': 'Dis', 'A': 'Aulus',\n",
        "        'Cn': 'Gnaeus', 'TI': 'Tiberius', 'S': 'Sextus', 'N': 'Numerius',\n",
        "    }\n",
        "\n",
        "    ABBREV_COMMON = {  # Common words (lowercase)\n",
        "        'a': 'animo', 'l': 'libens', 'v': 'votum', 'p': 'posuit',\n",
        "        's': 'sacrum', 'f': 'fecit', 'm': 'mensis', 'an': 'anno',\n",
        "        'ann': 'annorum', 'h': 'hic', 'e': 'est', 'pos': 'posuit',\n",
        "        't': 'tibi', 'd': 'de', 'sit': 'sit'\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def process(leiden_text):\n",
        "        \"\"\"Full pipeline: Leiden → clean transcription\"\"\"\n",
        "\n",
        "        # Step 1: Remove damage markers [3] (n unknown letters)\n",
        "        text = re.sub(r'\\[\\d+\\]', '', leiden_text)\n",
        "\n",
        "        # Step 2: Remove question marks and uncertain markers\n",
        "        text = re.sub(r'\\?', '', text)\n",
        "        text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)  # [text] → text\n",
        "\n",
        "        # Step 3: Join words broken across lines intelligently\n",
        "        # Handle patterns like \"Gem/ellian\" or \"ann]or/um\"\n",
        "        # Remove line breaks only when joining word fragments\n",
        "        text = re.sub(r'([a-z])/([a-z])', r'\\1\\2', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'(\\])/([a-z])', r'\\1\\2', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Step 4: Expand abbreviations with proper case handling\n",
        "        def expand_abbrev(match):\n",
        "            abbrev = match.group(1)\n",
        "            expansion = match.group(2) if match.group(2) else \"\"\n",
        "\n",
        "            # If expansion provided in parentheses, use it\n",
        "            if expansion:\n",
        "                # Keep expansion as-is, preserve case\n",
        "                return abbrev + expansion\n",
        "\n",
        "            # Try proper noun abbreviations first\n",
        "            if abbrev in LeidenProcessor.ABBREV_PROPER:\n",
        "                return LeidenProcessor.ABBREV_PROPER[abbrev]\n",
        "\n",
        "            # Try common abbreviations\n",
        "            if abbrev.lower() in LeidenProcessor.ABBREV_COMMON:\n",
        "                return LeidenProcessor.ABBREV_COMMON[abbrev.lower()]\n",
        "\n",
        "            # Return original if not found\n",
        "            return abbrev\n",
        "\n",
        "        # Pattern: X(expansion) captures abbreviation and optional expansion text\n",
        "        text = re.sub(r'([A-Za-z]+)\\(([^)]*)\\)', expand_abbrev, text)\n",
        "\n",
        "        # Step 5: Clean line break markers and multiple spaces\n",
        "        text = text.replace('/', ' ')\n",
        "        text = text.replace('\\\\', ' ')\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Step 6: Remove remaining brackets and junk\n",
        "        text = re.sub(r'[\\[\\]]', '', text)\n",
        "\n",
        "        # Step 7: Capitalize only first letter and proper nouns\n",
        "        # Simple heuristic: capitalize after space, but preserve lowercase articles/prepositions\n",
        "        LOWERCASE_WORDS = {'et', 'de', 'a', 'in', 'ex', 'ab'}\n",
        "\n",
        "        words = text.split()\n",
        "        result = []\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            if i == 0:  # First word always capitalized\n",
        "                result.append(word.capitalize())\n",
        "            elif word.lower() in LOWERCASE_WORDS:\n",
        "                result.append(word.lower())\n",
        "            elif word[0].isupper():  # Already capitalized (likely a name)\n",
        "                result.append(word)\n",
        "            else:\n",
        "                result.append(word.capitalize())\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "\n",
        "# Test it\n",
        "test_cases = [\n",
        "    \"Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]\",\n",
        "    \"D(is) M(anibus) s(acrum) / Severus / Tongini / an(norum) XXI / h(ic) s(itus) e(st)\",\n",
        "    \"Fig(lina) Mar/cian[a]\",\n",
        "    \"C[3] / N[3] / Pap(iria?) [ann]or/um L h(ic) s(itus) e(st)\",\n",
        "    \"Saturn/inus Bo/uti f(ilius) an(norum)\",\n",
        "]\n",
        "\n",
        "print(\"Updated Leiden processor:\\n\")\n",
        "for leiden in test_cases:\n",
        "    transcribed = LeidenProcessor.process(leiden)\n",
        "    print(f\"Leiden:        {leiden}\")\n",
        "    print(f\"Transcription: {transcribed}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRvdZi6Gyg2p",
        "outputId": "8c04c2b6-9b8c-49cd-c8cc-8d59b9e040c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Leiden processor:\n",
            "\n",
            "Leiden:        Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]\n",
            "Transcription: Iovi Optimo Maximo Quintus Cassius Cassianus Animo Libens Votum Posuit\n",
            "\n",
            "Leiden:        D(is) M(anibus) s(acrum) / Severus / Tongini / an(norum) XXI / h(ic) s(itus) e(st)\n",
            "Transcription: Dis Manibus Sacrum Severus Tongini Annorum XXI Hic Situs Est\n",
            "\n",
            "Leiden:        Fig(lina) Mar/cian[a]\n",
            "Transcription: Figlina Marciana\n",
            "\n",
            "Leiden:        C[3] / N[3] / Pap(iria?) [ann]or/um L h(ic) s(itus) e(st)\n",
            "Transcription: C N Papiria Annorum L Hic Situs Est\n",
            "\n",
            "Leiden:        Saturn/inus Bo/uti f(ilius) an(norum)\n",
            "Transcription: Saturninus Bouti Filius Annorum\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Load your CSV and process it\n",
        "import pandas as pd\n",
        "\n",
        "# Read CSV (adjust filename to match your upload)\n",
        "df = pd.read_csv('assets/actual_inscriptions.csv')  # or whatever your file is named\n",
        "\n",
        "print(f\"Loaded {len(df)} inscriptions\\n\")\n",
        "\n",
        "# Process all inscriptions\n",
        "records = []\n",
        "for idx, row in df.iterrows():\n",
        "    leiden_text = row.get('text', '') if isinstance(row, dict) else row['text']\n",
        "\n",
        "    if not leiden_text:\n",
        "        continue\n",
        "\n",
        "    transcription = LeidenProcessor.process(leiden_text)\n",
        "\n",
        "    record = {\n",
        "        \"id\": f\"actual_{idx}\",\n",
        "        \"text\": transcription,\n",
        "        \"leiden_source\": leiden_text,\n",
        "        \"annotations\": []\n",
        "    }\n",
        "    records.append(record)\n",
        "\n",
        "print(f\"Processed {len(records)} inscriptions\")\n",
        "\n",
        "# Show samples\n",
        "print(\"\\nFirst 3 samples:\")\n",
        "for rec in records[:3]:\n",
        "    print(f\"\\n  Leiden:        {rec['leiden_source']}\")\n",
        "    print(f\"  Transcription: {rec['text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK1UmicY0C6f",
        "outputId": "758ef6ef-a96f-48fe-ddfc-b4fb6d851bbc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 138 inscriptions\n",
            "\n",
            "Processed 138 inscriptions\n",
            "\n",
            "First 3 samples:\n",
            "\n",
            "  Leiden:        Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]\n",
            "  Transcription: Iovi Optimo Maximo Quintus Cassius Cassianus Animo Libens Votum Posuit\n",
            "\n",
            "  Leiden:        D(is) M(anibus) s(acrum) / Sycecale / v(ixit) an(n)o m(ensibus) [V] / soror[es] / Tricism[a] / Salcea / et Veget[a]\n",
            "  Transcription: Dis Manibus Sacrum Sycecale Vixit Anno Mensibus V Sorores Tricisma Salcea et Vegeta\n",
            "\n",
            "  Leiden:        Fig(lina) Gem/ellian[a]\n",
            "  Transcription: Figlina Gemelliana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Run model inference\n",
        "import spacy\n",
        "\n",
        "# Load model\n",
        "try:\n",
        "    nlp = spacy.load(\"training/model-best\")\n",
        "    print(f\"✅ Loaded model\")\n",
        "except OSError:\n",
        "    print(\"❌ Model not found at training/model-best\")\n",
        "    print(\"   Make sure the model folder exists and is in the correct location\")\n",
        "\n",
        "# Run predictions\n",
        "predictions = []\n",
        "\n",
        "for i, record in enumerate(records, 1):\n",
        "    text = record['text']\n",
        "\n",
        "    # Run model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract entities\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append({\n",
        "            \"text\": ent.text,\n",
        "            \"label\": ent.label_,\n",
        "            \"start\": ent.start_char,\n",
        "            \"end\": ent.end_char\n",
        "        })\n",
        "\n",
        "    pred = {\n",
        "        \"id\": record['id'],\n",
        "        \"leiden_source\": record['leiden_source'],\n",
        "        \"text\": text,\n",
        "        \"entities\": entities\n",
        "    }\n",
        "    predictions.append(pred)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"  Processed {i}/{len(records)}...\")\n",
        "\n",
        "print(f\"✅ Generated predictions for {len(predictions)} inscriptions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYMhnCpM0KfG",
        "outputId": "ee6ca97e-5b2b-465e-c9e8-5f011ef692f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded model\n",
            "  Processed 100/138...\n",
            "✅ Generated predictions for 138 inscriptions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Display predictions\n",
        "print(\"=\" * 100)\n",
        "print(\"PREDICTIONS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for i, pred in enumerate(predictions[:10], 1):\n",
        "    print(f\"\\n[{i}] {pred['leiden_source']}\")\n",
        "    print(f\"    Transcription: {pred['text']}\")\n",
        "    print(f\"    Entities:\")\n",
        "\n",
        "    if pred['entities']:\n",
        "        for ent in pred['entities']:\n",
        "            print(f\"      • {ent['text']:30} → {ent['label']}\")\n",
        "    else:\n",
        "        print(f\"      (none detected)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZEJcsAl0PG_",
        "outputId": "8ba0f775-9eee-48d6-80ba-f3e81d2496dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "PREDICTIONS\n",
            "====================================================================================================\n",
            "\n",
            "[1] Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]\n",
            "    Transcription: Iovi Optimo Maximo Quintus Cassius Cassianus Animo Libens Votum Posuit\n",
            "    Entities:\n",
            "      • Iovi Optimo Maximo             → DEDICATORY_FORMULA\n",
            "      • Quintus                        → PRAENOMEN\n",
            "      • Cassius                        → NOMEN\n",
            "      • Cassianus                      → COGNOMEN\n",
            "\n",
            "[2] D(is) M(anibus) s(acrum) / Sycecale / v(ixit) an(n)o m(ensibus) [V] / soror[es] / Tricism[a] / Salcea / et Veget[a]\n",
            "    Transcription: Dis Manibus Sacrum Sycecale Vixit Anno Mensibus V Sorores Tricisma Salcea et Vegeta\n",
            "    Entities:\n",
            "      • Dis Manibus                    → DEDICATORY_FORMULA\n",
            "      • Sacrum                         → NOMEN\n",
            "      • Mensibus                       → NOMEN\n",
            "\n",
            "[3] Fig(lina) Gem/ellian[a]\n",
            "    Transcription: Figlina Gemelliana\n",
            "    Entities:\n",
            "      (none detected)\n",
            "\n",
            "[4] D(is) M(anibus) s(acrum) / Severus / Tongini / an(norum) XXI / h(ic) s(itus) e(st) s(it) t(ibi) / t(erra) l(evis) m(ater)\n",
            "    Transcription: Dis Manibus Sacrum Severus Tongini Annorum XXI Hic Situs Est Sit Tibi Terra Levis Mater\n",
            "    Entities:\n",
            "      • Dis Manibus                    → DEDICATORY_FORMULA\n",
            "      • Sacrum                         → NOMEN\n",
            "      • Hic                            → DEDICATORY_FORMULA\n",
            "      • Sit Tibi                       → DEDICATORY_FORMULA\n",
            "\n",
            "[5] Iovi O(ptimo) M(aximo) / C(aius) A[3]/s M[3]/s v(otum) [s(olvit) l(ibens?) a(nimo?)]\n",
            "    Transcription: Iovi Optimo Maximo Caius As Ms Votum Solvit Libens Animo\n",
            "    Entities:\n",
            "      • Iovi Optimo Maximo             → DEDICATORY_FORMULA\n",
            "      • Caius                          → PRAENOMEN\n",
            "\n",
            "[6] Turaesio / Turcau/di f(ilius?) et / Pans[3] / Maeilonis / f(ilii?) s(it) eis / t(erra) l(evis)\n",
            "    Transcription: Turaesio Turcaudi Filius et Pans Maeilonis Filii Sit Eis Terra Levis\n",
            "    Entities:\n",
            "      (none detected)\n",
            "\n",
            "[7] Caeno / Loucin[i] / f(ilius) [\n",
            "    Transcription: Caeno Loucini Filius\n",
            "    Entities:\n",
            "      • Caeno                          → DEDICATORY_FORMULA\n",
            "\n",
            "[8] Saturn/inus Bo/uti f(ilius) an(norum) [\n",
            "    Transcription: Saturninus Bouti Filius Annorum\n",
            "    Entities:\n",
            "      • Annorum                        → COGNOMEN\n",
            "\n",
            "[9] an(norum)] / XVI h(ic) s(itus?) e(st) s(it) t(ibi) t(erra) / l(evis) Afelia avia / d(e) s(uo) f(ecit)\n",
            "    Transcription: Annorum XVI Hic Situs Est Sit Tibi Terra Levis Afelia Avia de Suo Fecit\n",
            "    Entities:\n",
            "      • Hic                            → DEDICATORY_FORMULA\n",
            "      • Sit Tibi                       → DEDICATORY_FORMULA\n",
            "\n",
            "[10] C[3] / N[3] / Pap(iria?) [ann]or/um L h(ic) s(itus) e(st) / t(ibi) t(erra) l(evis) [3] / [3]V[\n",
            "    Transcription: C N Papiria Annorum L Hic Situs Est Tibi Terra Levis V\n",
            "    Entities:\n",
            "      • C N                            → DEDICATORY_FORMULA\n",
            "      • Papiria                        → TRIBE\n",
            "      • Annorum                        → COGNOMEN\n",
            "      • Hic                            → DEDICATORY_FORMULA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Analyze statistics\n",
        "from collections import Counter\n",
        "\n",
        "label_counts = Counter()\n",
        "entity_counts = Counter()\n",
        "\n",
        "for pred in predictions:\n",
        "    for ent in pred['entities']:\n",
        "        label_counts[ent['label']] += 1\n",
        "        entity_counts[ent['text']] += 1\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"STATISTICS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nTotal inscriptions: {len(predictions)}\")\n",
        "print(f\"Total entities detected: {sum(len(p['entities']) for p in predictions)}\")\n",
        "\n",
        "print(f\"\\nEntities by label:\")\n",
        "for label, count in sorted(label_counts.items(), key=lambda x: -x[1]):\n",
        "    print(f\"  {label:25} {count:5}\")\n",
        "\n",
        "print(f\"\\nMost common entities:\")\n",
        "for entity, count in entity_counts.most_common(10):\n",
        "    print(f\"  {entity:30} {count:5}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zKL_6uL0T7z",
        "outputId": "3ee48117-7697-4c47-acd9-5616a9bc2c22"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "STATISTICS\n",
            "====================================================================================================\n",
            "\n",
            "Total inscriptions: 138\n",
            "Total entities detected: 311\n",
            "\n",
            "Entities by label:\n",
            "  DEDICATORY_FORMULA          147\n",
            "  NOMEN                        57\n",
            "  COGNOMEN                     47\n",
            "  PRAENOMEN                    38\n",
            "  TRIBE                         8\n",
            "  OCCUPATION                    8\n",
            "  FILIATION                     6\n",
            "\n",
            "Most common entities:\n",
            "  Sit Tibi                          34\n",
            "  Dis Manibus                       21\n",
            "  Hic                               19\n",
            "  Sacrum                            11\n",
            "  Hic Situs                          9\n",
            "  Annorum                            6\n",
            "  Filio                              6\n",
            "  Iovi Optimo Maximo                 5\n",
            "  Galeria                            5\n",
            "  Caius                              4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Save predictions to CSV (wide format)\n",
        "import pandas as pd\n",
        "\n",
        "# Define all possible labels\n",
        "LABELS = ['PRAENOMEN', 'NOMEN', 'COGNOMEN', 'TRIBE', 'EPITHET', 'DEDICATORY_FORMULA', 'ORIGIN', 'OCCUPATION','MILITARY_UNIT','AGE']\n",
        "\n",
        "# Flatten predictions into wide format\n",
        "export_data = []\n",
        "for pred in predictions:\n",
        "    row = {\n",
        "        'id': pred['id'],\n",
        "        'leiden_source': pred['leiden_source'],\n",
        "        'text': pred['text'],\n",
        "    }\n",
        "\n",
        "    # Add a column for each label with entities found\n",
        "    for label in LABELS:\n",
        "        entities_with_label = [e['text'] for e in pred['entities'] if e['label'] == label]\n",
        "        row[label] = ' | '.join(entities_with_label) if entities_with_label else ''\n",
        "\n",
        "    # Add total entity count\n",
        "    row['entity_count'] = len(pred['entities'])\n",
        "\n",
        "    export_data.append(row)\n",
        "\n",
        "# Create dataframe\n",
        "export_df = pd.DataFrame(export_data)\n",
        "\n",
        "# Reorder columns: metadata first, then labels, then count\n",
        "column_order = ['id', 'leiden_source', 'text'] + LABELS + ['entity_count']\n",
        "export_df = export_df[column_order]\n",
        "\n",
        "# Save\n",
        "export_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "print(\"✅ Saved predictions to predictions.csv\\n\")\n",
        "print(f\"Dimensions: {export_df.shape[0]} rows × {export_df.shape[1]} columns\\n\")\n",
        "print(export_df.head(10).to_string())\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download('predictions.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "G5Itx2Xo0YmQ",
        "outputId": "e5f591a0-34e5-491c-91c0-0950e14dc897"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved predictions to predictions.csv\n",
            "\n",
            "Dimensions: 138 rows × 14 columns\n",
            "\n",
            "         id                                                                                                              leiden_source                                                                                     text PRAENOMEN              NOMEN   COGNOMEN    TRIBE EPITHET            DEDICATORY_FORMULA ORIGIN OCCUPATION MILITARY_UNIT AGE  entity_count\n",
            "0  actual_0                               Iovi / Optimo / Maximo / Q(uintus) Cassius / Cassianus / a(nimo) l(ibens) [v(otum) p(osuit)]                   Iovi Optimo Maximo Quintus Cassius Cassianus Animo Libens Votum Posuit   Quintus            Cassius  Cassianus                             Iovi Optimo Maximo                                                 4\n",
            "1  actual_1        D(is) M(anibus) s(acrum) / Sycecale / v(ixit) an(n)o m(ensibus) [V] / soror[es] / Tricism[a] / Salcea / et Veget[a]      Dis Manibus Sacrum Sycecale Vixit Anno Mensibus V Sorores Tricisma Salcea et Vegeta            Sacrum | Mensibus                                               Dis Manibus                                                 3\n",
            "2  actual_2                                                                                                    Fig(lina) Gem/ellian[a]                                                                       Figlina Gemelliana                                                                                                                                        0\n",
            "3  actual_3  D(is) M(anibus) s(acrum) / Severus / Tongini / an(norum) XXI / h(ic) s(itus) e(st) s(it) t(ibi) / t(erra) l(evis) m(ater)  Dis Manibus Sacrum Severus Tongini Annorum XXI Hic Situs Est Sit Tibi Terra Levis Mater                       Sacrum                              Dis Manibus | Hic | Sit Tibi                                                 4\n",
            "4  actual_4                                       Iovi O(ptimo) M(aximo) / C(aius) A[3]/s M[3]/s v(otum) [s(olvit) l(ibens?) a(nimo?)]                                 Iovi Optimo Maximo Caius As Ms Votum Solvit Libens Animo     Caius                                                           Iovi Optimo Maximo                                                 2\n",
            "5  actual_5                             Turaesio / Turcau/di f(ilius?) et / Pans[3] / Maeilonis / f(ilii?) s(it) eis / t(erra) l(evis)                     Turaesio Turcaudi Filius et Pans Maeilonis Filii Sit Eis Terra Levis                                                                                                                                        0\n",
            "6  actual_6                                                                                             Caeno / Loucin[i] / f(ilius) [                                                                     Caeno Loucini Filius                                                                                  Caeno                                                 1\n",
            "7  actual_7                                                                                    Saturn/inus Bo/uti f(ilius) an(norum) [                                                          Saturninus Bouti Filius Annorum                                 Annorum                                                                                                1\n",
            "8  actual_8                      an(norum)] / XVI h(ic) s(itus?) e(st) s(it) t(ibi) t(erra) / l(evis) Afelia avia / d(e) s(uo) f(ecit)                  Annorum XVI Hic Situs Est Sit Tibi Terra Levis Afelia Avia de Suo Fecit                                                                         Hic | Sit Tibi                                                 2\n",
            "9  actual_9                             C[3] / N[3] / Pap(iria?) [ann]or/um L h(ic) s(itus) e(st) / t(ibi) t(erra) l(evis) [3] / [3]V[                                   C N Papiria Annorum L Hic Situs Est Tibi Terra Levis V                                 Annorum  Papiria                             C N | Hic                                                 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_30487992-3f5b-4774-8a75-c91a35a31951\", \"predictions.csv\", 24784)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r inscription_model.zip training/model-best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zYZrb4fQ8A4I",
        "outputId": "b6f4ead0-9711-4d3f-c8ca-bfc3ea34c6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: training/model-best/ (stored 0%)\n",
            "  adding: training/model-best/ner/ (stored 0%)\n",
            "  adding: training/model-best/ner/moves (deflated 74%)\n",
            "  adding: training/model-best/ner/model (deflated 8%)\n",
            "  adding: training/model-best/ner/cfg (deflated 33%)\n",
            "  adding: training/model-best/tokenizer (deflated 82%)\n",
            "  adding: training/model-best/vocab/ (stored 0%)\n",
            "  adding: training/model-best/vocab/key2row (stored 0%)\n",
            "  adding: training/model-best/vocab/vectors.cfg (deflated 28%)\n",
            "  adding: training/model-best/vocab/lookups.bin (stored 0%)\n",
            "  adding: training/model-best/vocab/strings.json (deflated 83%)\n",
            "  adding: training/model-best/vocab/vectors (deflated 10%)\n",
            "  adding: training/model-best/tok2vec/ (stored 0%)\n",
            "  adding: training/model-best/tok2vec/model (deflated 7%)\n",
            "  adding: training/model-best/tok2vec/cfg (stored 0%)\n",
            "  adding: training/model-best/meta.json (deflated 64%)\n",
            "  adding: training/model-best/config.cfg (deflated 60%)\n"
          ]
        }
      ]
    }
  ]
}