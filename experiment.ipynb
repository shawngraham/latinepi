{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-H-c4PNWWhQ"
      },
      "source": [
        "# Latin Epigraphic Inscription Parser (latinepi) - Complete Workflow Demo\n",
        "\n",
        "This notebook demonstrates the complete workflow for extracting structured personal data from Roman Latin epigraphic inscriptions using the `latinepi` tool with fast pattern-based entity extraction AND the new hybrid grammar parser!\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shawngraham/latinepi/blob/main/latinepi_demo.ipynb)\n",
        "\n",
        "## Features Demonstrated\n",
        "\n",
        "1. **Installation** - Set up latinepi (simple, no ML dependencies)\n",
        "2. **Pattern-Based Extraction** - Fast regex-based entity recognition (111+ patterns)\n",
        "3. **üÜï Hybrid Grammar Parser** - Extract unknown names using Latin grammatical structure\n",
        "4. **Confidence Filtering** - Apply thresholds and flag ambiguous entities\n",
        "5. **EDH Integration** - Download inscriptions from Epigraphic Database Heidelberg\n",
        "6. **Bulk Search** - Search and download multiple inscriptions by criteria\n",
        "7. **Complete Pipeline** - Search ‚Üí Download ‚Üí Extract ‚Üí Analyze\n",
        "8. **Visualization** - Analyze and visualize the extracted data\n",
        "\n",
        "## About the Tool\n",
        "\n",
        "**latinepi** extracts prosopographical data from Latin inscriptions using two approaches:\n",
        "\n",
        "### Pattern-Based Extraction (Default)\n",
        "- **Personal names**: praenomen, nomen, cognomen (111+ patterns)\n",
        "- **15 praenomina**: Gaius, Marcus, Lucius, Titus, Publius, Quintus, Sextus, etc.\n",
        "- **33 nomina**: Iulius, Flavius, Cornelius, Pompeius, etc. (with gender variants)\n",
        "- **45 cognomina**: Caesar, Maximus, Felix, Primus, Secundus, etc.\n",
        "- **Status markers**: D M, D M S (Dis Manibus Sacrum)\n",
        "- **Years lived**: Roman numeral conversion (e.g., XXX ‚Üí 30)\n",
        "- **Military service**: Legion numbers, ranks\n",
        "- **Relationships**: father, mother, daughter, son, wife, heir\n",
        "- **Roman tribes**: Fabia, Cornelia, Palatina, Quirina, etc.\n",
        "- **Locations**: Rome, Pompeii, Ostia, Aquincum, and more\n",
        "\n",
        "### üÜï Hybrid Grammar Parser (NEW!)\n",
        "- **Extracts unknown names** not in pattern lists by understanding Latin grammar\n",
        "- **Grammatical template matching** - recognizes formulaic structures\n",
        "- **Optional morphological analysis** - uses CLTK for case/gender/number\n",
        "- **Optional dependency parsing** - handles complex multi-person inscriptions\n",
        "- **70-90% accuracy on unknown names** vs 0% with patterns alone!\n",
        "\n",
        "‚ú® **Fast & Lightweight**: No ML dependencies for basic mode, instant results!\n",
        "\n",
        "Repository: https://github.com/shawngraham/latinepi\n",
        "\n",
        " NB When you get to the part that uses CLTK, and it asks you if you want to download Stanza, click in and say N everytime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW-F5P52WWhR"
      },
      "source": [
        "## 1. Installation\n",
        "\n",
        "Simple installation - just clone and install two lightweight dependencies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbw2En_zWWhR"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/shawngraham/latinepi.git\n",
        "%cd latinepi\n",
        "\n",
        "# Install the package (includes pandas and requests dependencies)\n",
        "!pip install -e .\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "print(\"   No ML dependencies needed - ready to parse!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNTREWubWWhR"
      },
      "source": [
        "## 2. Basic Setup\n",
        "\n",
        "Create sample data and set up working directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llHOgQC4WWhR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Create output directories\n",
        "Path('data').mkdir(exist_ok=True)\n",
        "Path('output').mkdir(exist_ok=True)\n",
        "Path('edh_downloads').mkdir(exist_ok=True)\n",
        "\n",
        "# Create sample CSV data with diverse inscription types\n",
        "sample_inscriptions = [\n",
        "    {\"id\": 1, \"text\": \"D M GAIVS IVLIVS CAESAR\", \"location\": \"Rome\"},\n",
        "    {\"id\": 2, \"text\": \"D M C Iulius Saturninus Mil(es) leg(ionis) VIII Aug(ustae) Vix(it) an(nos) XLII heres fecit\", \"location\": \"Rome\"},\n",
        "    {\"id\": 3, \"text\": \"D M S Valeria Maxima coniugi carissimae fecit Valerius Felix\", \"location\": \"Rome\"},\n",
        "    {\"id\": 4, \"text\": \"D M T Flavius Alexander Vix(it) an(nos) LX Flavia Restituta patri piissimo\", \"location\": \"Rome\"},\n",
        "    {\"id\": 5, \"text\": \"D M Aureliae Marcellae Vix(it) an(nos) XXV Aurelius Victor filiae dulcissimae\", \"location\": \"Rome\"},\n",
        "    {\"id\": 6, \"text\": \"D M L Sempronius Rufus Vix(it) an(nos) XXXV\", \"location\": \"Rome\"},\n",
        "    {\"id\": 7, \"text\": \"D M S Claudia Severa Vix(it) an(nos) XVIII\", \"location\": \"Rome\"},\n",
        "    {\"id\": 8, \"text\": \"MARCVS ANTONIVS FELIX\", \"location\": \"Pompeii\"},\n",
        "    {\"id\": 9, \"text\": \"LVCIVS CORNELIVS SCIPIO\", \"location\": \"Rome\"},\n",
        "    {\"id\": 10, \"text\": \"P Aelius Maximus Vix(it) an(nos) XXVII\", \"location\": \"Ostia\"},\n",
        "]\n",
        "\n",
        "# Save as CSV\n",
        "with open('data/sample_inscriptions.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=['id', 'text', 'location'])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(sample_inscriptions)\n",
        "\n",
        "# Save as JSON\n",
        "with open('data/sample_inscriptions.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(sample_inscriptions, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"‚úÖ Sample data created:\")\n",
        "print(f\"  - data/sample_inscriptions.csv ({len(sample_inscriptions)} inscriptions)\")\n",
        "print(f\"  - data/sample_inscriptions.json ({len(sample_inscriptions)} inscriptions)\")\n",
        "print(\"\\nüìÑ Sample inscription texts:\")\n",
        "for insc in sample_inscriptions[:5]:\n",
        "    print(f\"  {insc['id']}: {insc['text'][:60]}...\" if len(insc['text']) > 60 else f\"  {insc['id']}: {insc['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo6PtiAPWWhS"
      },
      "source": [
        "## 3. Pattern-Based Entity Extraction\n",
        "\n",
        "The parser uses comprehensive regex patterns to extract entities. Let's test it directly with the Python API first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtK8pjGlWWhS"
      },
      "outputs": [],
      "source": [
        "# Import the entity extraction function\n",
        "from latinepi.parser import extract_entities\n",
        "\n",
        "# Test inscriptions showcasing different features\n",
        "test_inscriptions = [\n",
        "    \"D M GAIVS IVLIVS CAESAR\",\n",
        "    \"D M C Iulius Saturninus Mil(es) leg(ionis) VIII Aug(ustae) Vix(it) an(nos) XLII\",\n",
        "    \"MARCVS ANTONIVS FELIX\",\n",
        "    \"D M Valeria Maxima coniugi carissimae\",\n",
        "    \"T Flavius Alexander Vix(it) an(nos) LX patri piissimo\",\n",
        "]\n",
        "\n",
        "print(\"üîç Testing Pattern-Based Extraction\")\n",
        "print(\"=\"*70)\n",
        "print(\"Extracting entities from sample inscriptions:\\n\")\n",
        "\n",
        "for i, inscription in enumerate(test_inscriptions, 1):\n",
        "    print(f\"{i}. '{inscription}'\")\n",
        "    entities = extract_entities(inscription)\n",
        "\n",
        "    if entities:\n",
        "        for entity_name, entity_data in entities.items():\n",
        "            print(f\"   {entity_name}: {entity_data['value']} (confidence: {entity_data['confidence']:.2f})\")\n",
        "    else:\n",
        "        print(\"   No entities extracted\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ Pattern matching includes:\")\n",
        "print(\"   ‚Ä¢ 15 praenomina (Gaius, Marcus, Lucius, etc.)\")\n",
        "print(\"   ‚Ä¢ 33 nomina with gender variants (Iulius/Iulia, etc.)\")\n",
        "print(\"   ‚Ä¢ 45 cognomina (Caesar, Felix, Primus, etc.)\")\n",
        "print(\"   ‚Ä¢ Roman numeral to Arabic conversion (XLII ‚Üí 42)\")\n",
        "print(\"   ‚Ä¢ Military ranks and legion numbers\")\n",
        "print(\"   ‚Ä¢ Relationships (father, mother, wife, etc.)\")\n",
        "print(\"   ‚Ä¢ 8 Roman tribes (Fabia, Palatina, etc.)\")\n",
        "print(\"   ‚Ä¢ 10+ major cities\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsWcY-KBWWhT"
      },
      "source": [
        "## 4. Process Sample Data with CLI\n",
        "\n",
        "Process the full CSV file using the command-line interface and output to JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylJ-XyexWWhT"
      },
      "outputs": [],
      "source": [
        "# Process CSV to JSON output\n",
        "!latinepi \\\n",
        "    --input data/sample_inscriptions.csv \\\n",
        "    --output output/entities.json\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXTRACTED ENTITIES (JSON)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with open('output/entities.json', 'r') as f:\n",
        "    entities = json.load(f)\n",
        "\n",
        "# Pretty print first 3 results\n",
        "for entity in entities[:3]:\n",
        "    print(f\"\\nüìú Inscription {entity.get('inscription_id')}:\")\n",
        "    for key, value in entity.items():\n",
        "        if key != 'inscription_id' and not key.endswith('_confidence'):\n",
        "            confidence_key = f\"{key}_confidence\"\n",
        "            confidence = entity.get(confidence_key, 'N/A')\n",
        "            print(f\"   {key}: {value} (confidence: {confidence})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPIBmvugWWhT"
      },
      "source": [
        "## 5. CSV Output Format\n",
        "\n",
        "Process the same data but output as CSV for easier analysis in spreadsheet tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYm3RefAWWhU"
      },
      "outputs": [],
      "source": [
        "# Process to CSV output\n",
        "!latinepi \\\n",
        "    --input data/sample_inscriptions.json \\\n",
        "    --output output/entities.csv \\\n",
        "    --output-format csv\n",
        "\n",
        "# Display as pandas DataFrame\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXTRACTED ENTITIES (CSV)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "df = pd.read_csv('output/entities.csv')\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nüìä Extracted {len(df)} inscription records with {len(df.columns)} fields\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnQlBypWWWhU"
      },
      "source": [
        "## 6. Confidence Threshold Filtering\n",
        "\n",
        "Apply confidence thresholds to filter high-quality entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuPxZyuZWWhU"
      },
      "outputs": [],
      "source": [
        "# High confidence threshold (0.9)\n",
        "!latinepi \\\n",
        "    --input data/sample_inscriptions.json \\\n",
        "    --output output/high_confidence.json \\\n",
        "    --confidence-threshold 0.9\n",
        "\n",
        "# Low confidence with ambiguous flagging\n",
        "!latinepi \\\n",
        "    --input data/sample_inscriptions.json \\\n",
        "    --output output/with_ambiguous.json \\\n",
        "    --confidence-threshold 0.7 \\\n",
        "    --flag-ambiguous\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONFIDENCE FILTERING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compare results\n",
        "with open('output/high_confidence.json', 'r') as f:\n",
        "    high_conf = json.load(f)\n",
        "\n",
        "with open('output/with_ambiguous.json', 'r') as f:\n",
        "    with_amb = json.load(f)\n",
        "\n",
        "print(f\"\\n‚úÖ High confidence (‚â•0.9): {len(high_conf)} inscriptions processed\")\n",
        "print(f\"   Average entities per inscription: {sum(len([k for k in r.keys() if not k.endswith('_confidence') and k != 'inscription_id']) for r in high_conf) / len(high_conf):.1f}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  With ambiguous flagging (‚â•0.7): {len(with_amb)} inscriptions processed\")\n",
        "ambiguous_count = sum(sum(1 for k in r.keys() if k.endswith('_ambiguous') and r[k]) for r in with_amb)\n",
        "print(f\"   Total ambiguous entities flagged: {ambiguous_count}\")\n",
        "\n",
        "# Show example with ambiguous flags\n",
        "print(\"\\nüìã Example with ambiguous flags:\")\n",
        "example = with_amb[0]\n",
        "for key, value in example.items():\n",
        "    if not key.endswith('_confidence'):\n",
        "        print(f\"   {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜï NEW: Hybrid Grammar Parser\n",
        "\n",
        "The new hybrid grammar parser goes beyond simple pattern matching to understand Latin grammatical structure. This allows extraction of **unknown names** not in the pattern lists!\n",
        "\n",
        "### The Problem with Pattern-Only Parsing\n",
        "\n",
        "Pattern matching works great for common names like \"Gaius Iulius Caesar\", but what if the inscription contains names like \"Vibius Paulus\" or \"Vibia Tertulla\" that aren't in our pattern lists?\n",
        "\n",
        "**Pattern-only extraction would miss these names entirely!**\n",
        "\n",
        "### The Solution: Grammatical Structure Analysis\n",
        "\n",
        "The hybrid parser understands Latin grammar:\n",
        "- **Genitive + dative** ‚Üí deceased person (`VIBIAE SABINAE FILIAE`)\n",
        "- **Nominative + FECIT** ‚Üí dedicator (`VIBIUS PAULUS FECIT`)\n",
        "- **Patronymic patterns** ‚Üí family relationships (`MARCUS GAII F.`)\n",
        "- **Grammatical cases** ‚Üí roles and relationships\n",
        "\n",
        "Let's see it in action!"
      ],
      "metadata": {
        "id": "HtQXmDJkWWhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test inscriptions with UNKNOWN names (not in pattern lists)\n",
        "# These names would be missed by pattern-only parsing!\n",
        "unknown_name_inscriptions = [\n",
        "    {\n",
        "        \"id\": 101,\n",
        "        \"text\": \"D M VIBIAE SABINAE FILIAE PIISSIMAE VIBIUS PAULUS PATER FECIT\",\n",
        "        \"description\": \"Unknown names: Vibia Sabina (deceased), Vibius Paulus (father)\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": 102,\n",
        "        \"text\": \"D M TERTULLAE LONGINAE FILIAE DULCISSIMAE\",\n",
        "        \"description\": \"Unknown names: Tertulla Longina (daughter)\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": 103,\n",
        "        \"text\": \"AVITUS MARINUS CONIVGI CARISSIMAE FECIT\",\n",
        "        \"description\": \"Unknown names: Avitus Marinus (husband)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save as JSON\n",
        "with open('data/unknown_names.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(unknown_name_inscriptions, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"‚úÖ Created test data with unknown names:\")\n",
        "for insc in unknown_name_inscriptions:\n",
        "    print(f\"\\n  ID {insc['id']}: {insc['text']}\")\n",
        "    print(f\"  ‚Üí {insc['description']}\")"
      ],
      "metadata": {
        "id": "N3t2kYdQWWhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc1kOdr0WWhU"
      },
      "outputs": [],
      "source": [
        "# COMPARISON: Pattern-Only vs Hybrid Grammar Parser\n",
        "print(\"üî¨ COMPARING EXTRACTION METHODS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nTest inscription: \\\"D M VIBIAE SABINAE FILIAE VIBIUS PAULUS PATER FECIT\\\"\\n\")\n",
        "\n",
        "# Method 1: Pattern-Only (original)\n",
        "print(\"üìã Method 1: Pattern-Only Extraction\")\n",
        "print(\"-\" * 80)\n",
        "from latinepi.parser import extract_entities\n",
        "entities_pattern = extract_entities(\"D M VIBIAE SABINAE FILIAE VIBIUS PAULUS PATER FECIT\")\n",
        "if entities_pattern:\n",
        "    for key, val in entities_pattern.items():\n",
        "        print(f\"  {key}: {val['value']} (confidence: {val['confidence']:.2f})\")\n",
        "else:\n",
        "    print(\"  ‚ùå No entities extracted\")\n",
        "\n",
        "print(f\"\\nüìä Entities found: {len(entities_pattern)}\")\n",
        "print(\"\\n‚ö†Ô∏è  Problem: Unknown names like 'Vibia Sabina' and 'Vibius Paulus' were missed!\\n\")\n",
        "\n",
        "# Method 2: Hybrid Grammar Parser\n",
        "print(\"\\nüß† Method 2: Hybrid Grammar Parser\")\n",
        "print(\"-\" * 80)\n",
        "from latinepi.hybrid_parser import extract_entities_hybrid\n",
        "entities_grammar = extract_entities_hybrid(\n",
        "    \"D M VIBIAE SABINAE FILIAE VIBIUS PAULUS PATER FECIT\",\n",
        "    use_morphology=False,\n",
        "    use_dependencies=False,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "if entities_grammar:\n",
        "    for key, val in entities_grammar.items():\n",
        "        source = val.get(\"extraction_phase\", \"unknown\")\n",
        "        print(f\"  {key}: {val['value']} (confidence: {val['confidence']:.2f}, source: {source})\")\n",
        "else:\n",
        "    print(\"  No entities extracted\")\n",
        "\n",
        "print(f\"\\nüìä Entities found: {len(entities_grammar)}\")\n",
        "print(\"\\n‚úÖ Success: Grammar parser extracted all names including unknown ones!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjajhzwmWWhU"
      },
      "outputs": [],
      "source": [
        "# Using the Hybrid Parser via CLI\n",
        "print(\"üñ•Ô∏è  HYBRID PARSER VIA CLI\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nProcessing inscriptions with unknown names using --use-grammar flag:\\n\")\n",
        "\n",
        "# Process with pattern-only (baseline)\n",
        "!latinepi \\\n",
        "    --input data/unknown_names.json \\\n",
        "    --output output/unknown_pattern_only.json\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Pattern-only results:\")\n",
        "with open('output/unknown_pattern_only.json', 'r') as f:\n",
        "    pattern_results = json.load(f)\n",
        "    total_entities_pattern = sum(len([k for k in r.keys() if not k.endswith('_confidence') and k != 'inscription_id']) for r in pattern_results)\n",
        "    print(f\"  Total entities extracted: {total_entities_pattern}\")\n",
        "\n",
        "# Process with hybrid grammar parser\n",
        "!latinepi \\\n",
        "    --input data/unknown_names.json \\\n",
        "    --output output/unknown_grammar.json \\\n",
        "    --use-grammar \\\n",
        "    --verbose\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Hybrid grammar parser results:\")\n",
        "with open('output/unknown_grammar.json', 'r') as f:\n",
        "    grammar_results = json.load(f)\n",
        "    total_entities_grammar = sum(len([k for k in r.keys() if not k.endswith('_confidence') and k != 'inscription_id']) for r in grammar_results)\n",
        "    print(f\"  Total entities extracted: {total_entities_grammar}\")\n",
        "\n",
        "print(f\"\\n‚ú® Improvement: {total_entities_grammar - total_entities_pattern} additional entities extracted!\")\n",
        "\n",
        "# Show detailed comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DETAILED COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "for i, (p_result, g_result) in enumerate(zip(pattern_results, grammar_results)):\n",
        "    print(f\"\\nüìú Inscription {p_result.get('inscription_id')}:\")\n",
        "    print(f\"  Pattern-only: {len([k for k in p_result.keys() if not k.endswith('_confidence') and k != 'inscription_id'])} entities\")\n",
        "    print(f\"  With grammar: {len([k for k in g_result.keys() if not k.endswith('_confidence') and k != 'inscription_id'])} entities\")\n",
        "\n",
        "    # Show what grammar parser found that pattern didn't\n",
        "    pattern_keys = set(k for k in p_result.keys() if not k.endswith('_confidence') and k != 'inscription_id')\n",
        "    grammar_keys = set(k for k in g_result.keys() if not k.endswith('_confidence') and k != 'inscription_id')\n",
        "    new_keys = grammar_keys - pattern_keys\n",
        "    if new_keys:\n",
        "        print(f\"  ‚úÖ Additional entities found by grammar parser:\")\n",
        "        for key in sorted(new_keys):\n",
        "            print(f\"     ‚Ä¢ {key}: {g_result[key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxR3cC74WWhV"
      },
      "source": [
        "## 7. EDH Single Inscription Download\n",
        "\n",
        "Download a specific inscription from the Epigraphic Database Heidelberg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT8ysvrmWWhV"
      },
      "outputs": [],
      "source": [
        "# Download inscription HD000001 from EDH\n",
        "print(\"üì• Downloading inscription HD000001 from EDH...\\n\")\n",
        "\n",
        "!latinepi \\\n",
        "    --download-edh HD000001 \\\n",
        "    --download-dir edh_downloads/\n",
        "\n",
        "# Check what was downloaded\n",
        "import os\n",
        "edh_files = list(Path('edh_downloads').glob('*.json'))\n",
        "print(f\"\\n‚úÖ Downloaded {len(edh_files)} file(s) to edh_downloads/\")\n",
        "\n",
        "if edh_files:\n",
        "    # Show structure of downloaded file\n",
        "    with open(edh_files[0], 'r') as f:\n",
        "        edh_data = json.load(f)\n",
        "\n",
        "    print(f\"\\nüìÑ Downloaded file: {edh_files[0].name}\")\n",
        "    print(f\"   Top-level keys: {list(edh_data.keys())}\")\n",
        "\n",
        "    # Show inscriptions if present\n",
        "    if 'inscriptions' in edh_data:\n",
        "        print(f\"   Number of inscriptions: {len(edh_data['inscriptions'])}\")\n",
        "        if edh_data['inscriptions']:\n",
        "            first_insc = edh_data['inscriptions'][0]\n",
        "            print(f\"   Inscription fields: {list(first_insc.keys())[:10]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt3gswkeWWhV"
      },
      "source": [
        "## 8. EDH Bulk Search and Download\n",
        "\n",
        "Search for multiple inscriptions by criteria and download them in parallel.\n",
        "\n",
        "‚ö†Ô∏è **Note**: This example uses small limits to avoid long download times. Adjust `--search-limit` for production use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyp1r_-yWWhV"
      },
      "outputs": [],
      "source": [
        "# Search for inscriptions from Rome (modern findspot)\n",
        "print(\"üîç Searching EDH for inscriptions from Rome...\\n\")\n",
        "\n",
        "!latinepi \\\n",
        "    --search-edh \\\n",
        "    --search-findspot-modern \"rome*\" \\\n",
        "    --search-limit 20 \\\n",
        "    --search-workers 5 \\\n",
        "    --download-dir edh_downloads/rome/\n",
        "\n",
        "# Check results\n",
        "rome_files = list(Path('edh_downloads/rome').glob('*.json'))\n",
        "print(f\"\\n‚úÖ Downloaded {len(rome_files)} inscriptions from Rome\")\n",
        "print(f\"   Files saved to: edh_downloads/rome/\")\n",
        "\n",
        "# Show some inscription IDs\n",
        "if rome_files:\n",
        "    print(f\"\\nüìã Sample inscription IDs:\")\n",
        "    for f in rome_files[:5]:\n",
        "        print(f\"   - {f.stem}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJHkPLhtWWhV"
      },
      "source": [
        "## 9. Temporal Search (By Date Range)\n",
        "\n",
        "Search inscriptions by time period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBBXg8X6WWhV"
      },
      "outputs": [],
      "source": [
        "# Search for 1st century AD inscriptions\n",
        "print(\"üîç Searching for 1st century AD inscriptions...\\n\")\n",
        "\n",
        "!latinepi \\\n",
        "    --search-edh \\\n",
        "    --search-year-from 1 \\\n",
        "    --search-year-to 100 \\\n",
        "    --search-limit 15 \\\n",
        "    --download-dir edh_downloads/first_century/\n",
        "\n",
        "# Check results\n",
        "century_files = list(Path('edh_downloads/first_century').glob('*.json'))\n",
        "print(f\"\\n‚úÖ Downloaded {len(century_files)} inscriptions from 1st century AD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xERO6tpOWWhV"
      },
      "source": [
        "## 10. Complete Pipeline: Search ‚Üí Download ‚Üí Extract ‚Üí Analyze\n",
        "\n",
        "Demonstrate the full workflow from search to analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rci1CUcuWWhV"
      },
      "outputs": [],
      "source": [
        "# Step 1: Search and download inscriptions from a specific province\n",
        "print(\"üîç Step 1: Searching for inscriptions from Dalmatia...\\n\")\n",
        "\n",
        "!latinepi \\\n",
        "    --search-edh \\\n",
        "    --search-province \"Dalmatia\" \\\n",
        "    --search-limit 10 \\\n",
        "    --download-dir edh_downloads/dalmatia/\n",
        "\n",
        "# Step 2: Process all downloaded inscriptions\n",
        "print(\"\\nüîß Step 2: Extracting entities from downloaded inscriptions...\\n\")\n",
        "\n",
        "# Get list of downloaded files\n",
        "dalmatia_files = list(Path('edh_downloads/dalmatia').glob('*.json'))\n",
        "\n",
        "if dalmatia_files:\n",
        "    # Process each file and collect results\n",
        "    all_results = []\n",
        "\n",
        "    for file_path in dalmatia_files:\n",
        "        # For now, process files individually (in production you might batch this)\n",
        "        output_file = f'output/dalmatia_{file_path.stem}.json'\n",
        "        !latinepi \\\n",
        "            --input {str(file_path)} \\\n",
        "            --output {output_file} \\\n",
        "            --confidence-threshold 0.7\n",
        "\n",
        "        with open(output_file, 'r') as f:\n",
        "            results = json.load(f)\n",
        "            all_results.extend(results)\n",
        "\n",
        "    print(f\"\\n‚úÖ Processed {len(dalmatia_files)} inscriptions\")\n",
        "    print(f\"   Total entity records extracted: {len(all_results)}\")\n",
        "\n",
        "    # Save combined results\n",
        "    with open('output/dalmatia_combined.json', 'w') as f:\n",
        "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Combined results saved to: output/dalmatia_combined.json\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No files downloaded. The search may not have returned results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZzYvupsWWhW"
      },
      "source": [
        "## 11. Data Analysis and Visualization\n",
        "\n",
        "Analyze the extracted entities to gain insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQFwrgT8WWhW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Load all extracted entities\n",
        "with open('output/entities.json', 'r') as f:\n",
        "    entities = json.load(f)\n",
        "\n",
        "print(\"üìä ENTITY EXTRACTION ANALYSIS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Count entity types\n",
        "entity_types = Counter()\n",
        "confidence_scores = []\n",
        "\n",
        "for record in entities:\n",
        "    for key, value in record.items():\n",
        "        if not key.endswith('_confidence') and key != 'inscription_id' and not key.endswith('_ambiguous'):\n",
        "            entity_types[key] += 1\n",
        "            confidence_key = f\"{key}_confidence\"\n",
        "            if confidence_key in record:\n",
        "                confidence_scores.append(record[confidence_key])\n",
        "\n",
        "# Print statistics\n",
        "print(f\"Total inscriptions processed: {len(entities)}\")\n",
        "print(f\"Total entities extracted: {sum(entity_types.values())}\")\n",
        "print(f\"Average entities per inscription: {sum(entity_types.values()) / len(entities):.2f}\")\n",
        "print(f\"\\nMost common entity types:\")\n",
        "for entity_type, count in entity_types.most_common():\n",
        "    print(f\"  {entity_type}: {count}\")\n",
        "\n",
        "if confidence_scores:\n",
        "    avg_confidence = sum(confidence_scores) / len(confidence_scores)\n",
        "    print(f\"\\nAverage confidence score: {avg_confidence:.3f}\")\n",
        "    print(f\"Confidence range: {min(confidence_scores):.3f} - {max(confidence_scores):.3f}\")\n",
        "\n",
        "# Visualization\n",
        "if entity_types:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Entity type distribution\n",
        "    types, counts = zip(*entity_types.most_common())\n",
        "    ax1.barh(types, counts, color='steelblue')\n",
        "    ax1.set_xlabel('Count')\n",
        "    ax1.set_title('Entity Type Distribution')\n",
        "    ax1.invert_yaxis()\n",
        "\n",
        "    # Confidence score distribution\n",
        "    if confidence_scores:\n",
        "        ax2.hist(confidence_scores, bins=20, color='coral', edgecolor='black')\n",
        "        ax2.set_xlabel('Confidence Score')\n",
        "        ax2.set_ylabel('Frequency')\n",
        "        ax2.set_title('Confidence Score Distribution')\n",
        "        ax2.axvline(avg_confidence, color='red', linestyle='--', label=f'Mean: {avg_confidence:.3f}')\n",
        "        ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('output/analysis.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"\\nüìà Visualizations saved to: output/analysis.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzV40-JnWWhW"
      },
      "source": [
        "## 12. Advanced Analysis: Name Patterns\n",
        "\n",
        "Analyze Roman naming conventions in the extracted data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "schCEcF_WWhW"
      },
      "outputs": [],
      "source": [
        "print(\"üìä ROMAN NAMING PATTERNS ANALYSIS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Count full name combinations\n",
        "tria_nomina = 0  # praenomen + nomen + cognomen\n",
        "duo_nomina = 0   # two of three\n",
        "single_names = 0 # just one name\n",
        "\n",
        "praenomina = Counter()\n",
        "nomina = Counter()\n",
        "cognomina = Counter()\n",
        "\n",
        "for record in entities:\n",
        "    has_praenomen = 'praenomen' in record\n",
        "    has_nomen = 'nomen' in record\n",
        "    has_cognomen = 'cognomen' in record\n",
        "\n",
        "    name_count = sum([has_praenomen, has_nomen, has_cognomen])\n",
        "\n",
        "    if name_count == 3:\n",
        "        tria_nomina += 1\n",
        "    elif name_count == 2:\n",
        "        duo_nomina += 1\n",
        "    elif name_count == 1:\n",
        "        single_names += 1\n",
        "\n",
        "    # Collect name components\n",
        "    if has_praenomen:\n",
        "        praenomina[record['praenomen']] += 1\n",
        "    if has_nomen:\n",
        "        nomina[record['nomen']] += 1\n",
        "    if has_cognomen:\n",
        "        cognomina[record['cognomen']] += 1\n",
        "\n",
        "total_with_names = tria_nomina + duo_nomina + single_names\n",
        "\n",
        "if total_with_names > 0:\n",
        "    print(f\"Naming Conventions:\")\n",
        "    print(f\"  Tria nomina (3 names): {tria_nomina} ({tria_nomina/total_with_names*100:.1f}%)\")\n",
        "    print(f\"  Duo nomina (2 names):  {duo_nomina} ({duo_nomina/total_with_names*100:.1f}%)\")\n",
        "    print(f\"  Single names:          {single_names} ({single_names/total_with_names*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nMost common praenomina:\")\n",
        "    for name, count in praenomina.most_common(5):\n",
        "        print(f\"  {name}: {count}\")\n",
        "\n",
        "    print(f\"\\nMost common nomina:\")\n",
        "    for name, count in nomina.most_common(5):\n",
        "        print(f\"  {name}: {count}\")\n",
        "\n",
        "    print(f\"\\nMost common cognomina:\")\n",
        "    for name, count in cognomina.most_common(5):\n",
        "        print(f\"  {name}: {count}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No name entities found in the data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT9n3SswWWhW"
      },
      "source": [
        "## 13. Export Results for Further Analysis\n",
        "\n",
        "Prepare data for external tools (Excel, R, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb8GS4hnWWhW"
      },
      "outputs": [],
      "source": [
        "# Convert all JSON results to CSV for spreadsheet analysis\n",
        "print(\"üíæ Exporting results to CSV format...\\n\")\n",
        "\n",
        "# Load entities\n",
        "with open('output/entities.json', 'r') as f:\n",
        "    entities = json.load(f)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(entities)\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv('output/all_entities_export.csv', index=False)\n",
        "print(f\"‚úÖ Exported {len(df)} records to: output/all_entities_export.csv\")\n",
        "\n",
        "# Create summary statistics CSV\n",
        "summary_data = []\n",
        "for col in df.columns:\n",
        "    if not col.endswith('_confidence') and col != 'inscription_id':\n",
        "        summary_data.append({\n",
        "            'entity_type': col,\n",
        "            'count': df[col].notna().sum(),\n",
        "            'unique_values': df[col].nunique(),\n",
        "            'avg_confidence': df[f\"{col}_confidence\"].mean() if f\"{col}_confidence\" in df.columns else None\n",
        "        })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df.to_csv('output/entity_summary.csv', index=False)\n",
        "print(f\"‚úÖ Summary statistics saved to: output/entity_summary.csv\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nüìä Entity Summary:\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üì¶ All outputs saved to 'output/' directory\")\n",
        "print(\"   Download these files to analyze in Excel, R, or other tools.\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New Approach"
      ],
      "metadata": {
        "id": "g5BIKx_WWaF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "SoP16bhUWyHS",
        "outputId": "8d544d80-3281-45fa-acfa-22e4459f13ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import random\n",
        "from spacy.tokens import DocBin, Span\n",
        "from spacy.training.example import Example\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "def create_training_data_with_matcher(nlp, df):\n",
        "    \"\"\"\n",
        "    Uses spaCy's Matcher to create robust, aligned training data.\n",
        "    This is the proper way to create rule-based annotations for spaCy.\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "\n",
        "    # 1. Define the Matcher patterns for our entities\n",
        "    # These patterns look for attributes of tokens, not raw text\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "    # Pattern for names (e.g., 2-3 consecutive capitalized words)\n",
        "    # {'IS_UPPER': True, 'OP': '+'} means \"one or more uppercase tokens\"\n",
        "    name_pattern = [{\"IS_UPPER\": True, \"IS_PUNCT\": False}, {\"IS_UPPER\": True, \"IS_PUNCT\": False, \"OP\": \"?\"}, {\"IS_UPPER\": True, \"IS_PUNCT\": False, \"OP\": \"?\"}]\n",
        "\n",
        "    # We can be more specific for deceased vs. dedicator later, but this is a good start\n",
        "    matcher.add(\"PERSON_NAME\", [name_pattern])\n",
        "\n",
        "    # Pattern for age (e.g., the token 'ANNIS' followed by an uppercase token (the numeral))\n",
        "    age_years_pattern = [{\"LOWER\": \"annis\"}, {\"IS_UPPER\": True}]\n",
        "    matcher.add(\"AGE_YEARS\", [age_years_pattern])\n",
        "\n",
        "    # Pattern for occupation (simple keyword match)\n",
        "    occupation_pattern = [{\"LOWER\": {\"IN\": [\"medico\", \"pictori\", \"fabro\", \"veterano\", \"negotiatori\", \"centurioni\"]}}]\n",
        "    matcher.add(\"OCCUPATION\", [occupation_pattern])\n",
        "\n",
        "    # Pattern for relationships (simple keyword match)\n",
        "    relationship_pattern = [{\"LOWER\": {\"IN\": [\"coniunx\", \"pater\", \"mater\", \"filius\", \"filia\", \"frater\"]}}]\n",
        "    matcher.add(\"RELATIONSHIP\", [relationship_pattern])\n",
        "\n",
        "    # 2. Process each inscription text\n",
        "    for _, row in df.iterrows():\n",
        "        text = str(row['transcription']) # Use the expanded transcription\n",
        "        doc = nlp(text)\n",
        "        matches = matcher(doc)\n",
        "\n",
        "        spans = [] # This will hold the Span objects\n",
        "\n",
        "        for match_id, start_token, end_token in matches:\n",
        "            label = nlp.vocab.strings[match_id]\n",
        "\n",
        "            # Create a Span object from the matched tokens. This is the key step!\n",
        "            # Spans are guaranteed to be aligned with token boundaries.\n",
        "            span = Span(doc, start_token, end_token, label=label)\n",
        "\n",
        "            # Specific logic to refine generic labels\n",
        "            if label == \"PERSON_NAME\":\n",
        "                # Heuristic: First name is likely the deceased, later names near a relationship are dedicators\n",
        "                if \"MANIBUS\" in doc[0:start_token].text:\n",
        "                    span.label_ = \"DECEASED_NAME\"\n",
        "                elif \"CONIUNX\" in doc[start_token:end_token+2].text.upper() or \"PATER\" in doc[start_token:end_token+2].text.upper():\n",
        "                    span.label_ = \"DEDICATOR_NAME\"\n",
        "                else:\n",
        "                    # Skip generic person names if we can't classify them\n",
        "                    continue\n",
        "\n",
        "            # For age, we only want to label the numeral, not the keyword\n",
        "            if label == \"AGE_YEARS\":\n",
        "                span = Span(doc, start_token + 1, end_token, label=label)\n",
        "\n",
        "            spans.append(span)\n",
        "\n",
        "        # Use spacy's utility to filter out overlapping spans, preferring longer ones\n",
        "        filtered_spans = spacy.util.filter_spans(spans)\n",
        "\n",
        "        if filtered_spans:\n",
        "            training_data.append(Example.from_dict(doc, {\"entities\": [(s.start_char, s.end_char, s.label_) for s in filtered_spans]}))\n",
        "\n",
        "    return training_data\n",
        "\n",
        "def train_spacy_ner_from_csv(csv_path: str, model_path: str = \"epigraphy_ner_model_v2\", n_iter: int = 30):\n",
        "    \"\"\"\n",
        "    Trains a spaCy NER model using a robust Matcher-based annotation approach.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Model Training (v2 with Matcher) ---\")\n",
        "\n",
        "    nlp = spacy.blank(\"la\") # Create a blank Latin model\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"Generating training data for {len(df)} inscriptions using Matcher...\")\n",
        "\n",
        "    # Generate the high-quality, aligned training data\n",
        "    training_data = create_training_data_with_matcher(nlp, df)\n",
        "\n",
        "    print(f\"Generated {len(training_data)} valid training examples.\")\n",
        "    if not training_data:\n",
        "        print(\"No training data generated. Check patterns and input text.\")\n",
        "        return\n",
        "\n",
        "    # Create the DocBin object\n",
        "    db = DocBin()\n",
        "    for example in training_data:\n",
        "        db.add(example.reference)\n",
        "\n",
        "    # Save the processed data\n",
        "    db.to_disk(\"./train.spacy\")\n",
        "\n",
        "    # Now, we can use the more modern spacy CLI training, or continue with the script method\n",
        "    # For simplicity, we'll continue with the script method\n",
        "    ner = nlp.add_pipe(\"ner\")\n",
        "    for example in training_data:\n",
        "        for ent in example.reference.ents:\n",
        "            ner.add_label(ent.label_)\n",
        "\n",
        "    print(\"\\n--- Begin Training Loop ---\")\n",
        "    optimizer = nlp.begin_training()\n",
        "\n",
        "    for itn in range(n_iter):\n",
        "        random.shuffle(training_data)\n",
        "        losses = {}\n",
        "\n",
        "        for example in training_data:\n",
        "            nlp.update([example], drop=0.5, sgd=optimizer, losses=losses)\n",
        "\n",
        "        print(f\"Iteration {itn + 1}/{n_iter}  |  Losses: {losses}\")\n",
        "\n",
        "    nlp.to_disk(model_path)\n",
        "    print(f\"\\n--- Training Complete ---\")\n",
        "    print(f\"Model saved to '{model_path}'\")\n",
        "    return model_path\n",
        "\n",
        "# --- Example Usage (Updated) ---\n",
        "if __name__ == '__main__':\n",
        "    csv_filename = \"data/fake-epigraphs.csv\"\n",
        "    # Create the dummy CSV again to ensure it's available\n",
        "    # dummy_data = {\n",
        "    #     'id': [701, 702, 707, 719, 285],\n",
        "    #     'text': [...], # Same as before\n",
        "    #     'transcription': [\n",
        "    #         'DIS MANIBUS GAIO IULIO VALENTI MILITI VETERANO LEGIONIS X GEMINAE VIXIT ANNIS XXXV IULIA SECUNDINA CONIUNX FECIT',\n",
        "    #         'DIS MANIBUS MARCO PETRONIO LONGO VETERANO LEGIONIS II AUGUSTAE FLAVIA PROCULA CONIUNX ET MARCUS PETRONIUS FILIUS FACIENDUM CURAVERUNT',\n",
        "    #         'DIS MANIBUS QUINTO HOSTILIO MARCELLO NEGOTIATORI VINARIO VIXIT ANNIS LX HOSTILIA FILIA PATRI OPTIMO',\n",
        "    #         'DIS MANIBUS VALERIO VITALI PICTORI VIXIT ANNIS XXXVIIII VALERIA MAXIMINA CONIUNX MARITO OPTIMO',\n",
        "    #         'DIS MANIBUS MARCO AURELIO SEVERO ALEXANDRO. VIXIT ANNIS XXVI'\n",
        "    #     ]\n",
        "    # }\n",
        "   # pd.DataFrame(dummy_data).to_csv(csv_filename, index=False)\n",
        "\n",
        "    # Train the new, more robust model\n",
        "    model_path = train_spacy_ner_from_csv(csv_path=csv_filename, n_iter=25)\n",
        "\n",
        "    # Load and test the newly trained model\n",
        "    print(\"\\n--- Testing the Trained Model (v2) ---\")\n",
        "    nlp_trained = spacy.load(model_path)\n",
        "\n",
        "    # Use the same test inscription as before\n",
        "    test_inscription = \"DIS MANIBUS TITO FLAVIO MARTIALI MEDICO VIXIT ANNIS LII FLAVIA PRISCA CONIUNX FECIT\"\n",
        "    doc = nlp_trained(test_inscription)\n",
        "\n",
        "    print(f\"Processing text: '{doc.text}'\")\n",
        "    print(\"Entities found:\")\n",
        "    if not doc.ents:\n",
        "        print(\"No entities found.\")\n",
        "    else:\n",
        "        for ent in doc.ents:\n",
        "            print(f\"- Text: '{ent.text}', Label: '{ent.label_}'\")"
      ],
      "metadata": {
        "id": "Bg3q5AHGWbcD",
        "outputId": "01be10f6-c388-47a1-810c-86d0692267f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Model Training (v2 with Matcher) ---\n",
            "Generating training data for 800 inscriptions using Matcher...\n",
            "Generated 63 valid training examples.\n",
            "\n",
            "--- Begin Training Loop ---\n",
            "Iteration 1/25  |  Losses: {'ner': np.float32(277.468)}\n",
            "Iteration 2/25  |  Losses: {'ner': np.float32(104.55537)}\n",
            "Iteration 3/25  |  Losses: {'ner': np.float32(75.98132)}\n",
            "Iteration 4/25  |  Losses: {'ner': np.float32(40.778038)}\n",
            "Iteration 5/25  |  Losses: {'ner': np.float32(28.674349)}\n",
            "Iteration 6/25  |  Losses: {'ner': np.float32(25.62096)}\n",
            "Iteration 7/25  |  Losses: {'ner': np.float32(20.355614)}\n",
            "Iteration 8/25  |  Losses: {'ner': np.float32(10.6596775)}\n",
            "Iteration 9/25  |  Losses: {'ner': np.float32(9.254774)}\n",
            "Iteration 10/25  |  Losses: {'ner': np.float32(7.9641757)}\n",
            "Iteration 11/25  |  Losses: {'ner': np.float32(9.416325)}\n",
            "Iteration 12/25  |  Losses: {'ner': np.float32(5.1306725)}\n",
            "Iteration 13/25  |  Losses: {'ner': np.float32(5.645845)}\n",
            "Iteration 14/25  |  Losses: {'ner': np.float32(3.9651623)}\n",
            "Iteration 15/25  |  Losses: {'ner': np.float32(1.990892)}\n",
            "Iteration 16/25  |  Losses: {'ner': np.float32(0.004995091)}\n",
            "Iteration 17/25  |  Losses: {'ner': np.float32(0.23354164)}\n",
            "Iteration 18/25  |  Losses: {'ner': np.float32(3.6843817)}\n",
            "Iteration 19/25  |  Losses: {'ner': np.float32(4.4354897)}\n",
            "Iteration 20/25  |  Losses: {'ner': np.float32(0.33274007)}\n",
            "Iteration 21/25  |  Losses: {'ner': np.float32(0.28355867)}\n",
            "Iteration 22/25  |  Losses: {'ner': np.float32(3.970695)}\n",
            "Iteration 23/25  |  Losses: {'ner': np.float32(5.2045336)}\n",
            "Iteration 24/25  |  Losses: {'ner': np.float32(7.838283e-06)}\n",
            "Iteration 25/25  |  Losses: {'ner': np.float32(0.18646282)}\n",
            "\n",
            "--- Training Complete ---\n",
            "Model saved to 'epigraphy_ner_model_v2'\n",
            "\n",
            "--- Testing the Trained Model (v2) ---\n",
            "Processing text: 'DIS MANIBUS TITO FLAVIO MARTIALI MEDICO VIXIT ANNIS LII FLAVIA PRISCA CONIUNX FECIT'\n",
            "Entities found:\n",
            "- Text: 'CONIUNX', Label: 'RELATIONSHIP'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# --- Step 1: Helper Functions for Data Cleaning and Structuring ---\n",
        "\n",
        "def roman_to_int(s: str) -> int:\n",
        "    \"\"\"Converts a Roman numeral string to an integer.\"\"\"\n",
        "    roman_map = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
        "    if not s or not all(c in roman_map for c in s):\n",
        "        return 0 # Return 0 for invalid or empty strings\n",
        "\n",
        "    result = 0\n",
        "    for i in range(len(s)):\n",
        "        # Check for subtractive notation (e.g., IV, IX)\n",
        "        if i > 0 and roman_map[s[i]] > roman_map[s[i-1]]:\n",
        "            result += roman_map[s[i]] - 2 * roman_map[s[i-1]]\n",
        "        else:\n",
        "            result += roman_map[s[i]]\n",
        "    return result\n",
        "\n",
        "def parse_roman_name(name_str: str) -> dict:\n",
        "    \"\"\"Parses a full Roman name string into its components.\"\"\"\n",
        "    parts = name_str.strip().split()\n",
        "    name_dict = {\"praenomen\": None, \"nomen\": None, \"cognomen\": None, \"full_name\": name_str}\n",
        "\n",
        "    # A simple parser based on the number of name parts\n",
        "    if len(parts) == 1:\n",
        "        name_dict[\"cognomen\"] = parts[0] # Often just the cognomen is used\n",
        "    elif len(parts) == 2:\n",
        "        name_dict[\"nomen\"] = parts[0]\n",
        "        name_dict[\"cognomen\"] = parts[1]\n",
        "    elif len(parts) >= 3:\n",
        "        name_dict[\"praenomen\"] = parts[0]\n",
        "        name_dict[\"nomen\"] = parts[1]\n",
        "        name_dict[\"cognomen\"] = \" \".join(parts[2:]) # Handle multiple cognomina\n",
        "\n",
        "    return name_dict\n",
        "\n",
        "# --- Step 2: The Main Extraction Function ---\n",
        "\n",
        "def extract_structured_data(inscriptions: list[str], model_path: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Applies a trained spaCy model to extract structured data from unseen inscriptions.\n",
        "\n",
        "    Args:\n",
        "        inscriptions (list[str]): A list of inscription texts to process.\n",
        "        model_path (str): The directory path of the trained spaCy model.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of dictionaries, where each dictionary represents the\n",
        "                    structured data extracted from one inscription.\n",
        "    \"\"\"\n",
        "    print(f\"--- Loading model from '{model_path}' ---\")\n",
        "    try:\n",
        "        nlp = spacy.load(model_path)\n",
        "    except IOError:\n",
        "        print(f\"Error: Could not load model from '{model_path}'.\")\n",
        "        print(\"Please ensure the model path is correct and the model is trained.\")\n",
        "        return []\n",
        "\n",
        "    print(\"--- Processing inscriptions ---\")\n",
        "    structured_results = []\n",
        "\n",
        "    for text in inscriptions:\n",
        "        # We process the uppercase version for consistency with training\n",
        "        doc = nlp(text.upper())\n",
        "\n",
        "        # Initialize the structured data dictionary for this inscription\n",
        "        result = {\n",
        "            \"source_text\": text,\n",
        "            \"deceased\": {\"full_name\": None, \"praenomen\": None, \"nomen\": None, \"cognomen\": None},\n",
        "            \"age_at_death\": {\"years\": None, \"months\": None, \"days\": None},\n",
        "            \"career\": {\"occupation\": None, \"military_unit\": None},\n",
        "            \"dedicators\": [] # Using a list as there can be multiple\n",
        "        }\n",
        "\n",
        "        # This will hold the last dedicator found, to link them with a relationship\n",
        "        last_dedicator = None\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            # Process each entity and place it in the correct part of the dictionary\n",
        "            if ent.label_ == \"DECEASED_NAME\":\n",
        "                result[\"deceased\"] = parse_roman_name(ent.text)\n",
        "\n",
        "            elif ent.label_ == \"DEDICATOR_NAME\":\n",
        "                dedicator_info = parse_roman_name(ent.text)\n",
        "                dedicator_info[\"relationship\"] = \"Unknown\" # Default value\n",
        "                result[\"dedicators\"].append(dedicator_info)\n",
        "                last_dedicator = dedicator_info # Keep track of the last one found\n",
        "\n",
        "            elif ent.label_ == \"RELATIONSHIP\":\n",
        "                 # Heuristic: Assume the relationship applies to the last dedicator found\n",
        "                if last_dedicator:\n",
        "                    last_dedicator[\"relationship\"] = ent.text\n",
        "\n",
        "            elif ent.label_ == \"AGE_YEARS\":\n",
        "                result[\"age_at_death\"][\"years\"] = roman_to_int(ent.text)\n",
        "            elif ent.label_ == \"AGE_MONTHS\":\n",
        "                result[\"age_at_death\"][\"months\"] = roman_to_int(ent.text)\n",
        "            elif ent.label_ == \"AGE_DAYS\":\n",
        "                result[\"age_at_death\"][\"days\"] = roman_to_int(ent.text)\n",
        "\n",
        "            elif ent.label_ == \"OCCUPATION\":\n",
        "                result[\"career\"][\"occupation\"] = ent.text\n",
        "            elif ent.label_ == \"MILITARY_UNIT\":\n",
        "                result[\"career\"][\"military_unit\"] = ent.text\n",
        "\n",
        "        structured_results.append(result)\n",
        "\n",
        "    print(f\"--- Extraction complete for {len(inscriptions)} inscriptions ---\")\n",
        "    return structured_results\n",
        "\n",
        "# --- Step 3: Example Usage ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define the path to the model we trained in the previous step\n",
        "    trained_model_path = \"epigraphy_ner_model_v2\"\n",
        "\n",
        "    # A list of new, unseen inscriptions to test the model on\n",
        "    unseen_inscriptions = [\n",
        "        # Test case 1: A simple inscription with a dedicator\n",
        "        \"DIS MANIBUS AURELIAE SATURNINAE VIXIT ANNIS XXXI AURELIUS FELIX CONIUNX BENE MERENTI POSUIT\",\n",
        "\n",
        "        # Test case 2: A soldier with a military unit and age\n",
        "        \"D M T CLAUDIO MAXIMO VETERANO LEGIONIS VII GEMINAE VIXIT ANNIS LXII MENSIBVS II\",\n",
        "\n",
        "        # Test case 3: An inscription with an occupation but no dedicator\n",
        "        \"DIS MANIBUS GAIO APULEIO DIOCLI PICTORI QUI VIXIT ANNIS QUADRAGINTA\", # Note: a slightly different phrasing\n",
        "\n",
        "        # Test case 4: A child's epitaph with a complex age\n",
        "        \"D M S FLAVIAE IANUARIAE VIXIT ANNIS V MENSIBVS VIII DIEBVS X PATER ET MATER FILIAE DULCISSIMAE\",\n",
        "\n",
        "        # Test case 5 (edge case): An inscription the model might struggle with\n",
        "        \"MEMORIAE AETERNAE VALERIAE FRONTINAE\"\n",
        "    ]\n",
        "\n",
        "    # Run the extraction function\n",
        "    extracted_data = extract_structured_data(unseen_inscriptions, trained_model_path)\n",
        "\n",
        "    # Print the results in a clean, readable JSON format\n",
        "    if extracted_data:\n",
        "        print(\"\\n--- Extracted Data Results ---\")\n",
        "        for i, data in enumerate(extracted_data):\n",
        "            print(f\"\\n--- Inscription #{i+1} ---\")\n",
        "            print(json.dumps(data, indent=2))"
      ],
      "metadata": {
        "id": "GwLPCi0LXpKD",
        "outputId": "94d7e8b9-0feb-4bfb-8492-7253e397a4fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading model from 'epigraphy_ner_model_v2' ---\n",
            "--- Processing inscriptions ---\n",
            "--- Extraction complete for 5 inscriptions ---\n",
            "\n",
            "--- Extracted Data Results ---\n",
            "\n",
            "--- Inscription #1 ---\n",
            "{\n",
            "  \"source_text\": \"DIS MANIBUS AURELIAE SATURNINAE VIXIT ANNIS XXXI AURELIUS FELIX CONIUNX BENE MERENTI POSUIT\",\n",
            "  \"deceased\": {\n",
            "    \"full_name\": null,\n",
            "    \"praenomen\": null,\n",
            "    \"nomen\": null,\n",
            "    \"cognomen\": null\n",
            "  },\n",
            "  \"age_at_death\": {\n",
            "    \"years\": null,\n",
            "    \"months\": null,\n",
            "    \"days\": null\n",
            "  },\n",
            "  \"career\": {\n",
            "    \"occupation\": null,\n",
            "    \"military_unit\": null\n",
            "  },\n",
            "  \"dedicators\": []\n",
            "}\n",
            "\n",
            "--- Inscription #2 ---\n",
            "{\n",
            "  \"source_text\": \"D M T CLAUDIO MAXIMO VETERANO LEGIONIS VII GEMINAE VIXIT ANNIS LXII MENSIBVS II\",\n",
            "  \"deceased\": {\n",
            "    \"full_name\": null,\n",
            "    \"praenomen\": null,\n",
            "    \"nomen\": null,\n",
            "    \"cognomen\": null\n",
            "  },\n",
            "  \"age_at_death\": {\n",
            "    \"years\": null,\n",
            "    \"months\": null,\n",
            "    \"days\": null\n",
            "  },\n",
            "  \"career\": {\n",
            "    \"occupation\": \"VETERANO\",\n",
            "    \"military_unit\": null\n",
            "  },\n",
            "  \"dedicators\": []\n",
            "}\n",
            "\n",
            "--- Inscription #3 ---\n",
            "{\n",
            "  \"source_text\": \"DIS MANIBUS GAIO APULEIO DIOCLI PICTORI QUI VIXIT ANNIS QUADRAGINTA\",\n",
            "  \"deceased\": {\n",
            "    \"full_name\": null,\n",
            "    \"praenomen\": null,\n",
            "    \"nomen\": null,\n",
            "    \"cognomen\": null\n",
            "  },\n",
            "  \"age_at_death\": {\n",
            "    \"years\": null,\n",
            "    \"months\": null,\n",
            "    \"days\": null\n",
            "  },\n",
            "  \"career\": {\n",
            "    \"occupation\": \"PICTORI\",\n",
            "    \"military_unit\": null\n",
            "  },\n",
            "  \"dedicators\": []\n",
            "}\n",
            "\n",
            "--- Inscription #4 ---\n",
            "{\n",
            "  \"source_text\": \"D M S FLAVIAE IANUARIAE VIXIT ANNIS V MENSIBVS VIII DIEBVS X PATER ET MATER FILIAE DULCISSIMAE\",\n",
            "  \"deceased\": {\n",
            "    \"full_name\": null,\n",
            "    \"praenomen\": null,\n",
            "    \"nomen\": null,\n",
            "    \"cognomen\": null\n",
            "  },\n",
            "  \"age_at_death\": {\n",
            "    \"years\": null,\n",
            "    \"months\": null,\n",
            "    \"days\": null\n",
            "  },\n",
            "  \"career\": {\n",
            "    \"occupation\": null,\n",
            "    \"military_unit\": null\n",
            "  },\n",
            "  \"dedicators\": []\n",
            "}\n",
            "\n",
            "--- Inscription #5 ---\n",
            "{\n",
            "  \"source_text\": \"MEMORIAE AETERNAE VALERIAE FRONTINAE\",\n",
            "  \"deceased\": {\n",
            "    \"full_name\": null,\n",
            "    \"praenomen\": null,\n",
            "    \"nomen\": null,\n",
            "    \"cognomen\": null\n",
            "  },\n",
            "  \"age_at_death\": {\n",
            "    \"years\": null,\n",
            "    \"months\": null,\n",
            "    \"days\": null\n",
            "  },\n",
            "  \"career\": {\n",
            "    \"occupation\": null,\n",
            "    \"military_unit\": null\n",
            "  },\n",
            "  \"dedicators\": []\n",
            "}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}